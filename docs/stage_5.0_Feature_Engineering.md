## 5 â€” Phase 5 Â· **Feature Engineering**<a name="5-phase-5--feature-engineering"></a>

> **Goal**: Create, transform, and augment features to improve model performance. All transforms run only on **train** (and val if needed), then applied to **test** to ensure no leakage.
> All â€œcolumn-craftingâ€ lives in **[`feature_engineering.py`](src/Feature%20Engineering/feature_engineering.py)**.  
> The `FeatureEngineer` class is a **buffet**: every classic transform is baked-in but
> disabled by defaultâ€”switch items on via kwargs or a small JSON/YAML config.

---

### 5Â·A Menu of Built-in Options<a name="5-phase-5--feature-engineering"></a>

| Category                      | Turn on with â‡¢                                                                | Notes                           |
| ----------------------------- | ----------------------------------------------------------------------------- | ------------------------------- | ------------------------------- | ------ | ------ | -------- | ------ | ------------------------- |
| **Numeric scalers**           | `numeric_scaler="standard                                                     | minmax                          | robust                          | maxabs | normal | quantile | none"` |                           |
| **Power / log**               | `numeric_power="yeo                                                           | boxcox                          | quantile"`Â·`log_cols=["price"]` |        |
| **Binning**                   | `quantile_bins={"age":4}` or `binning={"age":{"bins":5,"strategy":"kmeans"}}` |                                 |
| **Polynomial & interactions** | `polynomial_degree=2` Â· `interactions=True`                                   |                                 |
| **Rare grouping**             | `rare_threshold=0.01 # 1 %`                                                   | merges into `__rare__`          |
| **Cat encoders**              | `cat_encoder="onehot                                                          | ordinal                         | target                          | woe    | hash   | freq     | none"` | Target/WOE need `target=` |
| **Text vecs**                 | `text_vectorizer="tfidf                                                       | count                           | hashing"`Â·`text_cols=[â€¦]`       |        |
| **Datetime expand**           | `datetime_cols=[â€¦]`                                                           | Y/M/D/DOW/HR                    |
| **Cyclical sinâ€“cos**          | `cyclical_cols={"month":12,"dow":7}`                                          |                                 |
| **Date deltas**               | `date_delta_cols={"signup":"today"}`                                          | days-since                      |
| **Aggregations**              | `aggregations={"cust_id":["amt_mean","amt_sum"]}`                             | group-by roll-ups               |
| **SMOTE**                     | `sampler="smote"`                                                             | oversample during **fit**       |
| Custom plug-ins               | `custom_steps=[my_func]`                                                      | any `pd.DataFrameâ†’pd.DataFrame` |

---

### 5Â·B Quick Recipes

**Minimal**

```python
fe = FeatureEngineer(target="is_fraud").fit(df)
X  = fe.transform(df)
fe.save()  # âœ models/preprocessor.joblib
```

**Heavy stack**

```text
src/feature_engineering/feature_engineering.py
    FeatureEngineer(
        target="is_churn",
        numeric_scaler="robust",           # standard|minmax|robust|none
        numeric_power="yeo",               # yeo|boxcox|quantile|none
        log_cols=["revenue"],              # apply `log1p` to these columns
        quantile_bins={"age":4},           # 4 quantile bins for age
        polynomial_degree=2,               # generate secondâ€‘order polynomials/interactions
        rare_threshold=0.01,               # group categories with <1â€¯% frequency into â€œ__rare__â€
        cat_encoding="target",             # onehot|ordinal|target|woe|hash|freq|none
        text_vectorizer="tfidf",           # tfidf|count|hashing|none
        text_cols=["review"],              # columns to vectorize with text_vectorizer
        datetime_cols=["last_login"],       # for datetime expand (year/month/day/dow/hour)
        cyclical_cols={"hour":24},          # for hourâ†’two sin/cos columns
        date_delta_cols={"signup_date":"2020-01-01"},  # days since signup
        aggregations={"customer_id":["amt_mean","amt_sum"]},  # groupby features: mean and sum
        drop_nzv=True,                     # Phaseâ€‰4Â½ nearâ€‘zero variance prune (fast)
        corr_threshold=0.95,               # Phaseâ€‰4Â½ highâ€‘corr prune for numeric
        mi_quantile=0.10,                  # Phaseâ€‰4Â½ drop bottom 10â€¯% MI/Fâ€‘score
        custom_steps=[my_custom_func],     # arbitrary pd.DataFrameâ†’pd.DataFrame transforms
        save_path="models/preprocessor.joblib",
        report_dir="reports/feature"       # where to write feature_audit.json + shape
    )
```

```python
fe = FeatureEngineer(
        target="is_churn",
        numeric_scaler="robust",
        numeric_power="yeo",
        log_cols=["revenue"],
        quantile_bins={"age":4},
        cat_encoder="hash",
        rare_threshold=10,
        text_vectorizer="tfidf",
        text_cols=["review"],
        datetime_cols=["last_login"],
        cyclical_cols={"hour":24},
        polynomial_degree=2,
        sampler="smote"
        cat_encoding="target",
        text_vectorizer="tfidf",
        corr_threshold=.9,
        mi_quantile=.05
     ).fit(df, df.is_churn)
X = fe.transform(df); fe.save()
```

**CLI**

```bash
python -m Feature_Engineering.feature_engineering \
       --data data/processed/scaled.parquet \
       --target is_churn \
       --numeric_scaler robust \
       --log_cols revenue
```

---

### 5Â·C Artefacts <a name="5c-artifact--generated"></a>

| File                                   | Role                                       |
| -------------------------------------- | ------------------------------------------ |
| `models/preprocessor.joblib`           | Frozen transform pipeline (+SMOTE if used) |
| `models/preprocessor_manifest.json`    | SHA-256 + config snapshot                  |
| `reports/feature_shape.txt`            | Dense/-sparse shape & nnz %                |
| `reports/feature/feature_audit.json`   | n features before / after filtering        |
| `docs/feature_dictionary.md`           | human-readable feature dictionary          |
| `docs/feature_notes.yaml` _(optional)_ | hand-written blurbs                        |

---

### 5Â·D Custom Feature-Engineering Plug-ins<a name="5d-custom--advanced-plug-ins"></a>

Not every transform you need will fit the built-ins.
`FeatureEngineer` therefore accepts a list of **arbitrary callables**:

```python
custom_steps = [my_func1, my_func2, â€¦]   # each:  pd.DataFrame â†’ pd.DataFrame
```

They run **after** the standard ColumnTransformer, so they can read/write any
columns already produced by scaling, encoders, text vectors, etc.

#### Example â€“ domain ratios & log-tenure

```python
import numpy as np, pandas as pd
from Feature_Engineering.feature_engineering import FeatureEngineer

def add_ratios(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    df["spend_per_visit"] = df["total_spend"] / (df["num_visits"].clip(lower=1))
    df["log_tenure"]      = np.log1p(df["tenure_days"])
    return df

fe = FeatureEngineer(
        target="is_churn",
        numeric_scaler="standard",
        custom_steps=[add_ratios]
     ).fit(train_df, train_df.is_churn)

X_train = fe.transform(train_df)
fe.save()   # new columns now frozen into pre-processor
```

_Guidelines_

- Return **all original columns + new ones** (donâ€™t drop unless intentional).
- Keep it **pure**: no I/O, no global stateâ€”makes the pipeline portable.
- If you need parameters, wrap them in a closure or `functools.partial`.
- Add unit-tests in `tests/test_custom_steps.py` so the Phase-5 exit checklist
  can verify they still work after refactors.

> Once your custom step is serialised inside `preprocessor.joblib`, every model
> in Phase 6 will use it automaticallyâ€”no extra code paths to maintain.

### 5Â·E Automated **Feature Dictionary & Audit**<a name="5f-feature-dictionary"></a>

| â“ **Why bother?** | â€¢ New joiners instantly know what every column means.<br>â€¢ Reviewers & auditors can trace transformations and PII removal.<br>â€¢ CI can diff dictionaries and alert you when features silently disappear. |

> **Output** â€“ a single Markdown file<br> > **`docs/feature_dictionary.md`** regenerated on every Phase-5 run.

---

#### ğŸ”¨ How it is built & where it lives

1. **`feature_engineering.py`** writes a machine-readable summary
   to **`reports/feature/feature_audit.json`** each time you call `.save()`.
   It records:

   - origin column
   - transformation(s) applied
   - final dtype & whether it survived NZV / MI / filter rules

2. _(Optional)_ Curate human-friendly notes in
   **`docs/feature_notes.yaml`** (one-liners, units, caveats).

3. **`scripts/build_feature_dict.py`** merges the JSON + YAML and spits out
   the Markdown table.

   ```bash
   python scripts/build_feature_dict.py
   ```

The helper is invoked automatically at the end of the **Phase-5 CLI**
(`python -m Feature_Engineering.feature_engineering â€¦`), but you can run it
stand-alone if you edit notes.

4. CI / Git pre-commit can diff the generated file to catch sneaky feature
   drifts.

---

#### âœ¨ Sample snippet from the generated dictionary

```markdown
# ğŸ“– Feature Dictionary (63 columns)

| Feature          | Origin   | Transform      | Kept | Notes                                           |
| ---------------- | -------- | -------------- | ---- | ----------------------------------------------- |
| `income_log`     | income   | log1p          | âœ…   | Log of monthly income to mitigate right-skew.   |
| `cyc_month_sin`  | month    | cyclical (sin) | âœ…   | Sine component of calendar month (period = 12). |
| `zip_target_enc` | zip_code | target encode  | âœ…   | Smoothed target-encoding of 18 k ZIP codes.     |
| `cust_id`        | cust_id  | â€”              | âŒ   | Dropped â€“ identifier only.                      |
```

---

### 5.F Exit-check add-on (tick before Phase 6)

- [ ] `feature_dictionary.md` updated & committed
- [ ] Any newly _dropped_ or _added_ columns reviewed by a teammate
- [ ] Hand-written notes added for every new engineered feature
- [ ] Pipeline was fitted on **trainâ€¯+â€¯val** only (no test leakage).
- [ ] `preprocessor.joblib` is tracked by DVC or your model registry.
- [ ] `feature_shape.txt` logged (shape: `n_samples Ã— n_features_after_transform`).
- [ ] No silent column drops (all cat/text columns either encoded or passed through).
- [ ] All custom plugâ€‘in tests pass (`tests/test_custom_steps.py`).

---

#### 5.G Custom Featureâ€‘Engineering Plugâ€‘Ins

Any transform that doesnâ€™t fit the builtâ€‘ins can be added as a `custom_steps=[func1, func2, â€¦]`. Each `func` must be:

```python
def my_custom_func(df: pd.DataFrame) -> pd.DataFrame:
    # - MUST return all original columns + any NEW columns you want to add.
    # - NO I/O or global state inside the function. (Make it â€œpure.â€)
    # - If you need parameters, wrap them in a closure or use functools.partial.
    # Example: add domainâ€‘specific ratio features + log tenure:
    out = df.copy()
    out["spend_per_visit"] = out["total_spend"] / (out["num_visits"].clip(lower=1))
    out["log_tenure"]      = np.log1p(out["tenure_days"])
    return out
```

After the pipeline serializes your function in `models/preprocessor.joblib`, every model in Phaseâ€¯6 automatically uses itâ€”no extra code paths.

---

## ğŸ†• Phase 5Â·Â½ â€” **Baseline Benchmarking & & Pre-Processor Freeze** <a name="5.5-phase-baseline-freeze"></a>

> **Goal**: _Glue_ between **Feature Engineering** and **Model Design**.
> Freezes deterministic splits, prevents leakage, and sets a â€œbeat-thatâ€ baseline.

| Sub-step                          | Goal                                                        | Artefact(s)                                                    |
| --------------------------------- | ----------------------------------------------------------- | -------------------------------------------------------------- |
| **5Â·0 Train / Val / Test Split**  | Comparable, leak-free folds                                 | `data/splits/{train,val,test}.parquet`â€‚+â€‚`split_manifest.json` |
| **5Â·1 Stratification / Grouping** | Preserve class proportions or entity boundaries             | implemented inside **`split_and_baseline.py`**                 |
| **5Â·2 Baseline Model(s)**         | Majority-class, mean regressor, or random ranker            | `reports/baseline/baseline_metrics.json`                       |
| **5Â·3 Sanity Checks**             | Duplicate-row catch, leakage sniff, feature-drift check     | pipeline aborts on failure                                     |
| **5Â·4 Data-Pipeline Freeze**      | Persist the _fitted_ pre-processor used to build the splits | `models/preprocessor.joblib`â€‚+â€‚`preprocessor_manifest.json`    |

#### ğŸ“œ Code location

`src/Data Cleaning/split_and_baseline.py` â€“ single class **`SplitAndBaseline`**
(`fit â†’ split â†’ baseline â†’ checks â†’ freeze`).

```bash
# run end-to-end
python -m Data_Cleaning.split_and_baseline \
       --target is_churn \
       --stratify \
       --seed 42

```

```mermaid
flowchart TD
    A[0 Â· LOAD<br>processed.parquet] --> B[1 Â· STRAT / SPLIT]
    B --> C[2 Â· BASELINE<br>majority / mean]
    C --> D[3 Â· SANITY CHECKS]
    D --> E[4 Â· FREEZE PREPROCESSOR<br>+ SHA manifest]
```

The script:

1. Loads **`data/processed/scaled.parquet`**
2. Creates deterministic splits (stratified if flagged)
3. Computes & stores baseline metrics
4. Runs fast-fail leakage / duplication checks
5. Saves a SHA-stamped `preprocessor.joblib` + manifest

> **Exit criterion:** anyone can clone the repo, run `make baseline`,
> and reproduce the metrics within **Â± 0.01**.
> If the script fails, fix the issues before proceeding to Phase 6.

---

```

```
