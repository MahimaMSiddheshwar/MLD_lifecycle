````markdown
## 2Â·1 â€” Phase 2.1 Â· Probabilistic Analysis v2 <a name="2-phase-2.1--probabilistic-analysis-v2"></a>

> **Goal** â€” run a flexible, CLI-driven probabilistic analysis on your cleaned DataFrame in â€œautoâ€ or â€œfullâ€ mode. This stage fits parametric distributions, measures entropy and mutual information, builds conditional probability tables, applies probability-integral and quantile transforms, runs Bayesian group comparisons, and fits a Gaussian copula. All powered by **[`ProbabilisticAnalysis`](probabilistic_analysis_v2.py)**.

### Introduction

Before feature engineering, itâ€™s crucial to quantify the probabilistic structure of your data:

- **Distribution fitting** (AIC + KS-test) reveals the best parametric family for each numeric feature.
- **Shannon entropy** measures unpredictability in numeric or categorical data.
- **Mutual information** (or F-scores) ranks features by predictive power against the target.
- **Conditional probability tables** capture P(target|category) for each categorical feature.
- **Probability Integral Transform (PIT)** and **quantile transforms** normalize marginals for downstream modeling.
- **Bayesian group comparison** yields group means with 95% CIs.
- **Copula modeling** uncovers joint dependency structures independent of marginals.

Use `--mode auto` to skip sparse features, or `--mode full` to analyze every column.

---

### 2Â·1Â·0 What Happens Under the Hood ğŸ› 

1. **Argument Parsing**

   - `--data`: path to cleaned Parquet
   - `--outdir`: directory for all outputs
   - `--mode`: `auto` (threshold-based) vs. `full` (all columns)
   - `--target`, `--quantile-output`, `--min-dist-count`, `--entropy-bins`, `--jobs`

2. **Data Loading**

   - Reads `args.data` via `pd.read_parquet()`

3. **Distribution Fitting** (`fit_all_distributions`)

   - Candidate families: norm, lognorm, gamma, beta, weibull_min
   - Select best by AIC; compute KS stat & p-value
   - Save `best_fit_distributions.json`

4. **Shannon Entropy** (`shannon_entropy`)

   - Numeric features â†’ histogram bins; categoricals â†’ frequency bins
   - Save `shannon_entropy.csv`

5. **Mutual Information / F-score** (`mutual_info_scores`)

   - Classification MI if target has â‰¤10 classes; F-score regression otherwise
   - One-hot encode features; save `mutual_info_scores.csv`

6. **Conditional Probability Tables** (`conditional_probability_tables`)

   - For each object-dtype column, compute P(target|category)
   - Save `cpt_<column>.csv` per feature

7. **Probability Integral Transform** (`pit_transform`)

   - ECDF-based CDF values â†’ uniform marginals
   - Save `pit_transformed.csv`

8. **Quantile Transform** (`quantile_transform`)

   - Map numeric features to uniform or normal distribution
   - Save `quantile_transformed_normal.csv` or `quantile_transformed_uniform.csv`

9. **Bayesian Group Comparison** (`bayesian_group_comparison`)

   - For each categorical feature vs. numeric target: group mean + 95% CI
   - Save `bayesian_group_stats.csv`

10. **Copula Modeling** (`copula_modeling`)

    - Fit a `GaussianMultivariate` copula on numeric columns
    - Save `copula_params.json`

11. **Optional Diagnostics**
    - `qq_pp_plots(feature)` â†’ QQ & PP plots
    - `feature_importance()` â†’ permutation importance via RandomForest
    - `diagnostic_plots(feature)` â†’ histogram + fitted PDF

---

### ğŸ”§ Quick-Start

```bash
pip install pandas numpy scipy statsmodels scikit-learn copulas joblib
```
````

```bash
python probabilistic_analysis_v2.py \
  --data clean.parquet \
  --outdir reports/probabilistic_v2 \
  --mode auto \
  --target TARGET_COLUMN \
  --quantile-output normal \
  --min-dist-count 50 \
  --entropy-bins 20 \
  --jobs -1
```

---

### CLI Options

- `--data PATH` Path to cleaned Parquet file (required)
- `--outdir DIR` Directory to write JSON / CSV / plots (default: `reports/probabilistic_v2`)
- `--mode {auto,full}` `auto`: skip columns below count threshold; `full`: process all (default: `auto`)
- `--target NAME` Column name for target (enables MI, CPTs, Bayesian stats)
- `--quantile-output {normal,uniform}`
  Output distribution for quantile transform (default: `normal`)
- `--min-dist-count N` Min non-null values to fit distributions in `auto` (default: 50)
- `--entropy-bins N` Bins for numeric entropy (default: 20)
- `--jobs N` Parallel jobs for distribution fitting (default: all CPUs)

---

### Outputs & Directory Structure

```
reports/probabilistic_v2/
â”œâ”€ best_fit_distributions.json
â”œâ”€ shannon_entropy.csv
â”œâ”€ mutual_info_scores.csv         # only if --target provided
â”œâ”€ cpt_<feature>.csv â€¦            # one per categorical feature
â”œâ”€ pit_transformed.csv
â”œâ”€ quantile_transformed_<mode>.csv
â”œâ”€ bayesian_group_stats.csv       # only if --target provided
â”œâ”€ copula_params.json
â”œâ”€ qqpp_<feature>.png             # when run manually
â”œâ”€ diagnostic_<feature>.png       # when run manually
```

---

> **Next up âœ Phase 2Â·2 Â· Data Preparation & Feature Engineering**

```

```
