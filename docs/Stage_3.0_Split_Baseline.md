````markdown
## 3Â·0 â€” Phase 3 Â· Split, Multi-Baseline & Preprocessor Freeze <a name="3-phase-3--split-baseline"></a>

> **Goal** â€” take your cleaned & processed dataset, split into train/val/test (with optional SMOTE), train and evaluate multiple dummy baseline models, pick and record the winning baseline (with checksums), run basic sanity checks, and freeze the numeric preprocessor for downstream pipelines.

### Introduction

After youâ€™ve ingested, validated, and explored your data, itâ€™s time to establish a reproducible baseline and lock in your preprocessing.  
Phase 3 ensures that:

- Your data splits are deterministic and, if desired, stratified or oversampled to handle class imbalance.
- Multiple naive â€œdummyâ€ models (mean/median for regression, most_frequent/stratified/uniform for classification) are trained and scored, so you always have a sanity-check performance floor.
- The best performing baseline is saved with its checksum for auditability, alongside a full metrics report for all candidates.
- Fundamental sanity checks (no leakage or duplicate rows) guard against common pipeline mistakes.
- The numeric preprocessor (StandardScaler) is fitted on the full dataset and frozen for consistent feature transformations in later phases.

---

### 3Â·0Â·0 What Happens Under the Hood ðŸ› 

1. **Split Data** (`split_data`)

   - 80/10/10 train/val/test split, with optional stratification on the target
   - Optional SMOTE oversampling on the training fold only (classification)
   - Persist `train.parquet`, `val.parquet`, `test.parquet` under `data/splits/`
   - Write `split_manifest.json` with seed, stratify, oversample flags and row counts

2. **Build Baseline Models** (`build_baseline`)

   - **Regression** candidates: DummyRegressor strategies `mean`, `median`, `quantile`
   - **Classification** candidates: DummyClassifier strategies `most_frequent`, `stratified`, `uniform`
   - Fit each on training set, evaluate on validation set (MAE & RÂ² for regression; accuracy & F1 for classification)
   - Save each model under `models/baselines/` and compute its SHA-256 checksum
   - Generate `baseline_metrics.json` summarizing all candidates and their scores
   - Write `baseline_manifest.json` pointing to the winning model and checksum

3. **Sanity Checks** (`sanity_checks`)

   - Ensure no duplicate rows across train/test splits (index-based)
   - Detect any feature column identical to the target (simple leakage sniff)

4. **Freeze Preprocessor** (`freeze_preprocessor`)
   - Fit a `Pipeline([("scale", StandardScaler())])` on all numeric columns of the processed dataset
   - Save `models/preprocessor.joblib` and record its SHA-256 in `preprocessor_manifest.json`

---

### ðŸ”§ Quick-Start

```bash
pip install pandas numpy scikit-learn imbalanced-learn joblib
```
````

```bash
python split_and_baseline.py \
  --target TARGET_COLUMN \
  [--seed 42] \
  [--stratify] \
  [--oversample]
```

---

### CLI Options

- `--target NAME` **(required)** target column name
- `--seed INT` random seed for reproducibility (default: 42)
- `--stratify` stratify splits by target distribution
- `--oversample` apply SMOTE to training fold (classification only)

---

### Outputs & Directory Structure

```
data/
â””â”€ processed/scaled.parquet         # input processed data
data/
â””â”€ splits/
   â”œâ”€ train.parquet
   â”œâ”€ val.parquet
   â”œâ”€ test.parquet
   â””â”€ split_manifest.json

reports/
â””â”€ baseline/
   â””â”€ baseline_metrics.json

models/
â”œâ”€ baselines/
â”‚  â”œâ”€ mean_regressor.joblib
â”‚  â”œâ”€ median_regressor.joblib
â”‚  â”œâ”€ quantile0.25_regressor.joblib
â”‚  â”œâ”€ most_frequent_clf.joblib
â”‚  â”œâ”€ stratified_clf.joblib
â”‚  â””â”€ uniform_clf.joblib
â”‚  â””â”€ baseline_manifest.json
â””â”€ preprocessor.joblib
â””â”€ preprocessor_manifest.json
```

---

> **Next up âžœ Phase 4 Â· Data Preparation & Feature Engineering**

```

```
