# ─────────────────────────────────────────────────────────────────────────────
# dvc.yaml ─ Entire MLDLC pipeline (with Probabilistic Analysis)
# ------------------------------------------------------------------------------

stages:
  # ────────────────────── PHASE-2 · DATA COLLECTION ───────────────────────────
  collect:
    cmd: |
      python -m data_ingest.omni_cli ${params.collect.args}
    deps:
      - src/data_ingest/data_collector.py
      - src/data_ingest/omni_cli.py
      - params.yaml
    outs:
      - data/raw/ # immutable snapshot(s)

  # ────────────────────── PHASE-3 · DATA PREPARATION ─────────────────────────
  prepare:
    cmd: |
      python -m ml_pipeline.prepare \
        ${params.prepare.args}
    deps:
      - data/raw/ # any raw files used in params
      - src/ml_pipeline/prepare.py
      - params.yaml
    outs:
      - data/interim/clean.parquet # cleaned data
      - data/processed/scaled.parquet # scaled/imputed data
    metrics:
      - reports/lineage/prep_manifest.json:
          cache: false

  # ────────────────────── PHASE-4 · CORE EDA ─────────────────────────────────
  eda_core:
    cmd: |
      python -m Data_Analysis.EDA \
        --mode all \
        --data=data/interim/clean.parquet \
        --target=${params.target}
    deps:
      - data/interim/clean.parquet
      - src/Data_Analysis/EDA.py
      - params.yaml
    outs-persist:
      - reports/eda/ # core EDA CSVs/PNGs collected here

  # ────────────────────── PHASE-4 · ADVANCED EDA ──────────────────────────────
  eda_adv:
    cmd: |
      python -m Data_Analysis.EDA_advance \
        --data=data/interim/clean.parquet \
        --target=${params.target}
    deps:
      - data/interim/clean.parquet
      - src/Data_Analysis/EDA_advance.py
      - params.yaml
    outs-persist:
      - reports/eda/advanced/ # advanced EDA outputs

  # ──────────────── PHASE-4½ · PROBABILISTIC ANALYSIS ─────────────────────────
  prob_analysis:
    cmd: |
      python -m data_analysis.probabilistic_analysis \
        --data=data/interim/clean.parquet \
        --target=${params.target} \
        --impute_method=${params.probabilistic.impute_method} \
        ${params.probabilistic.do_pit:+--do_pit} \
        ${params.probabilistic.do_quantile:+--do_quantile} \
        ${params.probabilistic.do_copula:+--do_copula}
    deps:
      - data/interim/clean.parquet
      - src/data_analysis/probabilistic_analysis.py
      - src/data_analysis/probabilistic_analysis.py # your class definition
      - params.yaml
    outs:
      - reports/probabilistic/ # distributions.json, entropy.csv, mutual_info.csv, etc.
    metrics:
      - reports/probabilistic/distributions.json:
          cache: false
      - reports/probabilistic/entropy.csv:
          cache: false
      - reports/probabilistic/mutual_info.csv:
          cache: false

  # ────────────────── PHASE-4½ · FEATURE SELECTION & EARLY SPLIT ───────────────
  feat_select:
    cmd: |
      python -m Feature_Selection.feature_select \
        --data=data/processed/scaled.parquet \
        --target=${params.target} \
        ${params.feature_selection.args}
    deps:
      - data/processed/scaled.parquet
      - src/Feature_Selection/feature_select.py
      - params.yaml
    outs:
      - data/processed/selected.parquet
    metrics:
      - reports/feature/feature_audit.json:
          cache: false

  split_baseline:
    cmd: |
      python -m Data_Cleaning.split_and_baseline \
        --data=data/processed/selected.parquet \
        --target=${params.target} \
        --seed=${params.split.seed} \
        --stratify=${params.split.stratify} \
        --oversample=${params.split.oversample}
    deps:
      - data/processed/selected.parquet
      - src/Data_Cleaning/split_and_baseline.py
      - models/preprocessor.joblib
      - params.yaml
    outs:
      - data/splits/train.parquet
      - data/splits/val.parquet
      - data/splits/test.parquet
      - data/splits/split_manifest.json
      - models/preprocessor_manifest.json
    metrics:
      - reports/baseline/baseline_metrics.json:
          cache: false

  # ───────────────────── PHASE-5 · FEATURE ENGINEERING ────────────────────────
  feat_engineer:
    cmd: |
      python -m Feature_Engineering.feature_engineering \
        --data=data/processed/selected.parquet \
        --target=${params.target} \
        ${params.feature_engineering.args}
    deps:
      - data/processed/selected.parquet
      - src/Feature_Engineering/feature_engineering.py
      - params.yaml
    outs:
      - models/preprocessor.joblib
      - reports/feature/feature_shape.txt
    metrics:
      - reports/feature/feature_audit.json:
          cache: false

  # ───────────────────── PHASE-5½ · BASELINE ONLY (TRAIN vs. TEST) ────────────
  baseline:
    cmd: |
      python -m Data_Cleaning.split_and_baseline \
        --baseline-only \
        --train=data/splits/train.parquet \
        --test=data/splits/test.parquet \
        --target=${params.target}
    deps:
      - data/splits/train.parquet
      - data/splits/test.parquet
      - models/preprocessor.joblib
      - src/Data_Cleaning/split_and_baseline.py
      - params.yaml
    outs:
      - reports/baseline/baseline_metrics.json
      - models/preprocessor_manifest.json

  # ───────────────────── PHASE-6 · MODEL TRAINING & TUNING ─────────────────────
  train:
    cmd: |
      python -m model.train \
        --train=data/splits/train.parquet \
        --val=data/splits/val.parquet \
        --test=data/splits/test.parquet \
        --preprocessor=models/preprocessor.joblib \
        --config=params.yaml
    deps:
      - data/splits/train.parquet
      - data/splits/val.parquet
      - data/splits/test.parquet
      - models/preprocessor.joblib
      - src/model/train.py
      - params.yaml
    outs:
      - models/model.pkl
      - models/model_card.md
    metrics:
      - reports/metrics/train_metrics.json:
          cache: false
    plots:
      - reports/metrics/loss_curve.csv:
          x: epoch
          y: loss

  # ───────────────────── PHASE-7 · EVALUATION (ON TEST SET) ───────────────────
  evaluate:
    cmd: |
      python -m model.evaluate \
        --test=data/splits/test.parquet \
        --model=models/model.pkl \
        --config=params.yaml
    deps:
      - data/splits/test.parquet
      - models/model.pkl
      - src/model/evaluate.py
      - params.yaml
    metrics:
      - reports/metrics/test_metrics.json:
          cache: false
    plots:
      - reports/metrics/roc_curve.csv:
          template: simple

  # ───────────────────── PHASE-8 · PACKAGE & EXPORT ───────────────────────────
  package:
    cmd: |
      python -m model.package \
        --model=models/model.pkl \
        --out=artefacts/model.onnx
    deps:
      - models/model.pkl
      - src/model/package.py
      - params.yaml
    outs:
      - artefacts/model.onnx

  # ───────────────────── PHASE-9 · DEPLOY (OPTIONAL) ──────────────────────────
  deploy:
    cmd: |
      bash deploy/push_to_registry.sh artefacts/model.onnx
    deps:
      - artefacts/model.onnx
      - deploy/push_to_registry.sh
    outs:
      - deploy/last_deploy.txt
# ─────────────────────────────────────────────────────────────────────────────

# Notes on Usage:
#
#  • To run the entire pipeline from scratch (or to reproduce any downstream stage),
#      > dvc repro
#
#  • To run just a specific stage, e.g. Probabilistic Analysis only:
#      > dvc repro prob_analysis
#
#  • To run through Feature Selection & Splitting (skipping EDA + Probabilistic):
#      > dvc repro feat_select split_baseline
#
#  • To train a new model after changing code, without re‐running EDA/probabilistic/etc:
#      > dvc repro train evaluate package
#
#  • Make sure `params.yaml` contains all referenced keys (see next section).
