**Data Cleaning** and **Feature Transformation** both live in the preprocessing stage of an ML pipeline, but they serve distinct purposes:

---

### ğŸ“‹ Data Cleaning (PhaseÂ 3)

**Goal:**â€¯Make your raw data â€œcorrect and consistentâ€ so that downstream algorithms wonâ€™t choke on invalid inputs.

Typical tasks include:

1. **Handling Missing Values**

   - _Identify_ which columns/rows have gaps.
   - _Decide_ to drop, impute (median/mode/KNN/MICE), or flag them.
   - _Example_: If â€œageâ€ is null for 2â€¯% of records, you might impute with median.

2. **Deduplication & Invariants**

   - _Remove_ exactâ€duplicate rows.
   - _Drop_ columns with zero or nearâ€zero variance (e.g. a column that is â€œTRUEâ€ for 99.9â€¯% of rows).
   - _Example_: If â€œuser_idâ€ appears twice with identical data, keep only one.

3. **Type & Format Enforcement**

   - _Convert_ strings like `"2023-01-01"` into `datetime` objects.
   - _Coerce_ numericâ€looking strings (`"42.0"`) into floats/ints.
   - _Validate_ ranges or categories via a schema (Pandera, Cerberus, etc.).
   - _Example_: Ensure `â€œsalaryâ€` is a float â‰¥â€¯0; any negative values get flagged.

4. **Outlier Detection & Treatment**

   - _Flag_ or _remove_ points beyond 3â€¯Ïƒ, outside IQRâ€¯Ã—â€¯1.5, or via Isolation Forest.
   - Decide whether to cap/floor, drop, or leave them for later.
   - _Example_: A transaction amount of \$1â€¯M when the 99th percentile is \$1â€¯000 likely is a typo.

5. **Basic Value Normalization (sometimes overlaps)**

   - _Strip_ leading/trailing whitespace, lowercase all categorical text, remove nonâ€‘ASCII.
   - _Standardize_ date formats (e.g. â€œMM/DD/YYYYâ€ â†’ ISO).
   - _Example_: Convert â€œNYCâ€ vs â€œNew York Cityâ€ into a single canonical category.

6. **Logging & Lineage**

   - _Record_ which rows/columns were dropped or imputed, with timestamps and checksums.
   - _Version_ the cleaned snapshot (e.g. `data/interim/clean.parquet`) so you can reproduce exactly.

_If you imagine â€œData Cleaningâ€ as making the dataset technically validâ€”no NaNs where they shouldnâ€™t be, no nonsensical values, no duplicated keysâ€”then youâ€™re on the right track._

---

### ğŸ”„ Feature Transformation (PhaseÂ 5)

**Goal:**â€¯Take your â€œcleanedâ€ data and convert/augment it into representations that are more predictive for your algorithms.

Key activities include:

1. **Scaling & Normalization**

   - _Scale_ numerical columns via StandardScaler, MinMax, RobustScaler, PowerTransformer, or QuantileTransformer.
   - _Logâ€‘transform_ strongly skewed distributions (`np.log1p`).
   - _Example_: Apply `StandardScaler` to â€œageâ€ and â€œincomeâ€ so they both have zero meanâ€¯/â€¯unit variance.

2. **Encoding Categorical Variables**

   - _Oneâ€‘hot_ for lowâ€‘cardinality text columns.
   - _Ordinal_ if thereâ€™s a natural order (e.g. â€œlowâ€, â€œmediumâ€, â€œhighâ€ â†’ 0,â€¯1,â€¯2).
   - _Target/WOE/Frequency/Hash_ encoding for highâ€‘cardinality categories.
   - _Example_: Use TargetEncoder on â€œzipcodeâ€ (hundreds of unique values) to avoid blowing up feature space.

3. **Binning / Discretization**

   - _Quantile bins_ or _kâ€‘means bins_ (e.g. group â€œageâ€ into 4 bins).
   - _Custom cutpoints_ if you know domain thresholds (e.g. â€œincomeâ€Â <â€¯30â€¯k, 30â€“60â€¯k, >â€¯60â€¯k).
   - _Example_: `KBinsDiscretizer(n_bins=5, strategy='quantile')` to bucket a continuous variable.

4. **Creating Derived or Interaction Features**

   - _Ratios_: e.g. `spend_per_visit = total_spend / num_visits`.
   - _Polynomials_: e.g. adding `ageÂ²` or any crossâ€‘term `heightâ€¯Ã—â€¯weight`.
   - _Dateâ€‘based_: extract year/month/dayâ€ofâ€week or compute â€œdays_since_signupâ€.
   - _Example_: If you have â€œorder_dateâ€ and â€œsignup_dateâ€, create `days_since_signup = order_date â€“ signup_date`.

5. **Text Vectorization (lightâ€weight)**

   - _CountVectorizer_ or _TFâ€‘IDF_ for short text fields (e.g. product reviews).
   - _Text length_ / _word count_ as numeric features if the text is very short.
   - _Example_: Build a TFâ€‘IDF matrix for â€œreview_textâ€ with a cap of 100â€¯tokens.

6. **Dimensionality Reduction (optional within FE)**

   - _PCA_, _UMAP_, or _TruncatedSVD_ to reduce large sparse text/oneâ€‘hot matrices.
   - _Example_: Apply `TruncatedSVD(n_components=20)` on TFâ€‘IDF vectors to get 20 components.

7. **Imbalance Adjustment (embedded in FE)**

   - _SMOTE_ or _NearMiss_ applied only on training set after transformations.
   - _(Note: Rarely done as part of FE workflow, but often integrated in the pipeline fit step.)_
   - _Example_: If â€œfraudâ€ ratio is 1â€¯% positive, run SMOTE to synthetically oversample minority after encoding.

8. **Custom Plugâ€‘ins & Expert Features**

   - Any domainâ€‘specific transform you write (e.g. â€œextract sentiment from commentsâ€).
   - Unit tests to ensure these transforms always return expected DataFrame shapes.
   - _Example_: A function `add_sentiment_score(df)` that uses a preâ€‘trained sentiment lexicon to output a new column.

9. **Feature Pruning & Audit**

   - _Nearâ€‘zero variance_ filtered before heavy transforms.
   - _Correlation filter_ (drop one of highly correlated pair).
   - _Mutual information filter_ (drop features below bottomâ€¯10â€¯% MI).
   - _Feature audits_: record number of features in/out and sparsity.
   - _Example_: Run `VarianceThreshold(1eâ€‘5)` on numeric columns to drop constant or almost constant features.

_In short: Feature Transformation is about â€œreshaping and enrichingâ€ the cleansed data into numerical arrays (or sparse matrices) your model can digest, while optionally pruning irrelevant or redundant features._

---

### ğŸ”‘ Where the Line Blurs

- **â€œIs imputing missing values data cleaning or feature transformation?â€**
  Â­â€”â€¯Itâ€™s **data cleaning** because youâ€™re ensuring no NaNs remain.
- **â€œIs `np.log1p(column)` cleaning or transforming?â€**
  Â­â€”â€¯Itâ€™s strictly **feature transformation**, since youâ€™re reshaping a valid numeric column for better modeling.
- **â€œDropping a column because it has 99.9â€¯% a single valueâ€”where does that live?â€**
  Â­â€”â€¯It can appear in **data cleaning** (as an invariant/dedup step) or in **feature selection** (pruning a useless predictor). In practice, most pipelines treat varianceâ€based drops as part of â€œcleaning,â€ but you can also argue itâ€™s an FEâ€pruning step.

---

### ğŸ—ï¸ Typical Workflow Placement

```text
PhaseÂ 3 â€“ Data Cleaning:
  â€¢ Deduplicate rows
  â€¢ Enforce schemas & dtypes (Pandera)
  â€¢ Handle missing (drop / medianâ€mode / KNN)
  â€¢ Detect & treat outliers (IQR / zscore / IF)
  â€¢ Normalize text (lowercase, strip whitespace)
  â€¢ Save â€œcleanedâ€ snapshot â†’ data/interim/clean.parquet

PhaseÂ 4 â€“ EDA & Feature Selection:
  â€¢ Run univariate, bivariate, multivariate analyses
  â€¢ Identify leaky or useless features (perfect corr with target)
  â€¢ Drop irrelevant columns (IDs, future timestamps)
  â€¢ Early split (train/test) if desired to prevent leakage
  â€¢ Save â€œselectedâ€ features â†’ data/processed/selected.parquet

PhaseÂ 5 â€“ Feature Engineering:
  â€¢ Scale & powerâ€transform numeric columns
  â€¢ Encode categorical columns (oneâ€hot, target, etc.)
  â€¢ Bin numeric columns (quantile/kmeans)
  â€¢ Create derived features (ratios, date deltas, interactions)
  â€¢ Vectorize light text fields (TFâ€‘IDF + text length)
  â€¢ Prune lowâ€‘MI / highly correlated features
  â€¢ Optionally oversample (SMOTE) on train set only
  â€¢ Produce final preprocessor â†’ models/preprocessor.joblib
```

---

### ğŸ“Œ Summary

- **Data Cleaning**â€¯= fix â€œbadâ€ data (missing, invalid, duplicate, outlier).
- **Feature Transformation**â€¯= take â€œgoodâ€ data and reshape or augment it (scale, encode, derive new columns) to maximize predictive signal.

Keeping these separate ensures you (a) never leak test information into your transforms, and (b) maintain a clear lineage: one snapshot for â€œcleanedâ€ data, another for â€œengineeredâ€ features.
