{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Machine Learning Preprocessing Pipeline\n",
        "\n",
        "This notebook provides a template for a standard machine learning preprocessing pipeline. It includes:\n",
        "1. Setup & Imports\n",
        "2. Loading data from CSV\n",
        "3. Missing Value Analysis & Imputation\n",
        "4. Basic Cleaning (duplicates, constant features, etc.)\n",
        "5. Train/Validation/Test Split\n",
        "6. Outlier Detection\n",
        "7. Feature Selection\n",
        "8. Next Steps (Modeling)\n",
        "\n",
        "Feel free to modify paths, parameters, and classes as needed for your project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup & Imports\n",
        "Import all necessary libraries, define helper classes/functions if not already installed.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Standard libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Sklearn & imblearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Custom modules (adjust import paths as needed)\n",
        "# from stage2_imputer import Stage2Imputer\n",
        "# from stage3_outlier_detection import OutlierDetector\n",
        "# from stage4_scaling_transformation import NumericTransformer\n",
        "# from split_and_baseline import SplitAndBaseline\n",
        "\n",
        "# Display settings\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "sns.set(style=\"whitegrid\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Data\n",
        "Read your raw CSV data into a pandas DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# TODO: Update the path to your CSV file\n",
        "DATA_PATH = 'data/raw/your_data.csv'\n",
        "\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 Quick Profile\n",
        "- Dimensions\n",
        "- Data types\n",
        "- Missing value summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Shape\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "\n",
        "# Data types\n",
        "df.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Missing values\n",
        "missing_summary = df.isna().mean().sort_values(ascending=False)\n",
        "missing_summary.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Missing Value Analysis & Imputation\n",
        "Use a dedicated imputer (e.g., Stage2Imputer) or simple strategies here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Example: Using Stage2Imputer (uncomment once imported)\n",
        "# imputer = Stage2Imputer(max_missing_frac_drop=0.9, knn_neighbors=5, verbose=True)\n",
        "# df_imputed = imputer.fit_transform(df)\n",
        "# df_imputed.head()\n",
        "\n",
        "# For demonstration, a simple approach:\n",
        "df_imputed = df.copy()\n",
        "# Numeric median imputation\n",
        "for col in df_imputed.select_dtypes(include=[np.number]).columns:\n",
        "    df_imputed[col].fillna(df_imputed[col].median(), inplace=True)\n",
        "# Categorical mode imputation\n",
        "for col in df_imputed.select_dtypes(include=['object', 'category']).columns:\n",
        "    df_imputed[col].fillna(df_imputed[col].mode()[0], inplace=True)\n",
        "\n",
        "df_imputed.isna().sum().sum()  # Should be zero if no missing remain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Basic Cleaning\n",
        "- Remove duplicates\n",
        "- Remove constant or near-constant features\n",
        "- High cardinality checks, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_clean = df_imputed.copy()\n",
        "\n",
        "# 4.1 Drop duplicate rows\n",
        "before_dup = len(df_clean)\n",
        "df_clean.drop_duplicates(inplace=True)\n",
        "print(f\"Dropped {before_dup - len(df_clean)} duplicate rows\")\n",
        "\n",
        "# 4.2 Remove constant / near-constant columns (e.g., > 99% same value)\n",
        "constant_cols = [col for col in df_clean.columns \n",
        "                 if df_clean[col].nunique(dropna=False) / len(df_clean) < 0.01]\n",
        "df_clean.drop(columns=constant_cols, inplace=True)\n",
        "print(f\"Dropped {len(constant_cols)} constant/near-constant columns: {constant_cols}\")\n",
        "\n",
        "# 4.3 Optional: Remove high-cardinality categorical features\n",
        "high_card_cols = [col for col in df_clean.select_dtypes(include=['object', 'category']).columns \n",
        "                  if df_clean[col].nunique() > 100]\n",
        "# df_clean.drop(columns=high_card_cols, inplace=True)\n",
        "print(f\"High-cardinality columns (consider review): {high_card_cols}\")\n",
        "\n",
        "df_clean.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Train/Validation/Test Split\n",
        "Split your data, optionally apply oversampling on training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Specify target column\n",
        "TARGET = 'your_target_column'\n",
        "\n",
        "X = df_clean.drop(columns=[TARGET])\n",
        "y = df_clean[TARGET]\n",
        "\n",
        "# 80/20 train + temp split\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y if y.dtype != 'float' else None\n",
        ")\n",
        "\n",
        "# 50/50 validation + test split from temp\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp if y_temp.dtype != 'float' else None\n",
        ")\n",
        "\n",
        "# Optional: SMOTE oversampling on training (classification only)\n",
        "if y_train.dtype != 'float':  # classification\n",
        "    sm = SMOTE(random_state=42)\n",
        "    X_train_res, y_train_res = sm.fit_resample(X_train, y_train)\n",
        "    X_train, y_train = X_train_res, y_train_res\n",
        "\n",
        "print(f\"X_train: {X_train.shape}, X_val: {X_val.shape}, X_test: {X_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Outlier Detection\n",
        "Use the `OutlierDetector` class or a custom approach to identify and treat outliers in the training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Example: Using OutlierDetector (uncomment once imported)\n",
        "# outlier_detector = OutlierDetector(outlier_threshold=3, robust_covariance=True, cap_outliers=True)\n",
        "# train_df = pd.concat([X_train, y_train], axis=1)\n",
        "# train_clean = outlier_detector.fit_transform(train_df, numeric_cols=list(X_train.select_dtypes(include=[np.number]).columns))\n",
        "# X_train_clean = train_clean.drop(columns=[TARGET])\n",
        "# y_train_clean = train_clean[TARGET]\n",
        "\n",
        "# For demonstration: Detect using simple Z-score threshold on numeric features\n",
        "X_train_num = X_train.select_dtypes(include=[np.number]).copy()\n",
        "z_scores = np.abs((X_train_num - X_train_num.mean()) / X_train_num.std())\n",
        "outlier_mask = (z_scores > 3).any(axis=1)\n",
        "print(f\"Detected {outlier_mask.sum()} outliers by simple Z-score method.\")\n",
        "# Optionally drop or cap these rows\n",
        "X_train_clean = X_train[~outlier_mask]\n",
        "y_train_clean = y_train[~outlier_mask]\n",
        "print(f\"After dropping: X_train_clean: {X_train_clean.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Feature Selection\n",
        "- Remove low-variance features\n",
        "- Remove strongly correlated features\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 7.1 Remove low-variance features (VarianceThreshold)\n",
        "selector = VarianceThreshold(threshold=0.01)  # features with <1% variance\n",
        "selector.fit(X_train_clean)\n",
        "low_variance_cols = X_train_clean.columns[~selector.get_support()].tolist()\n",
        "print(f\"Low-variance columns to drop: {low_variance_cols}\")\n",
        "X_train_fs = X_train_clean.drop(columns=low_variance_cols)\n",
        "X_val_fs = X_val.drop(columns=low_variance_cols)\n",
        "X_test_fs = X_test.drop(columns=low_variance_cols)\n",
        "\n",
        "# 7.2 Remove highly correlated features\n",
        "corr_matrix = X_train_fs.corr().abs()\n",
        "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
        "high_corr_cols = [col for col in upper_tri.columns if (upper_tri[col] > 0.9).any()]\n",
        "print(f\"Highly correlated columns to drop: {high_corr_cols}\")\n",
        "X_train_fs.drop(columns=high_corr_cols, inplace=True)\n",
        "X_val_fs.drop(columns=high_corr_cols, inplace=True)\n",
        "X_test_fs.drop(columns=high_corr_cols, inplace=True)\n",
        "\n",
        "print(f\"Final feature set size: {X_train_fs.shape[1]} features\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Next Steps: Modeling\n",
        "- Build and evaluate your machine learning models here.\n",
        "- Example: logistic regression, random forest, etc.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Example placeholder:\n",
        "# from sklearn.linear_model import LogisticRegression\n",
        "# model = LogisticRegression()\n",
        "# model.fit(X_train_fs, y_train_clean)\n",
        "# preds = model.predict(X_val_fs)\n",
        "# from sklearn.metrics import accuracy_score\n",
        "# print(\"Validation accuracy:\", accuracy_score(y_val, preds))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}