{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Example ML Pipeline Interactive Notebook\n",
        "This notebook demonstrates running the example Python scripts from `pipeline/`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*Imported from `pipeline.zip` example*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Workflow Overview\n",
        "# What This Script Does\n",
        "\n",
        "1. **Phase\u00a02 \u2014 Data Collection**\n",
        "\n",
        "   - Runs\n",
        "\n",
        "     ```bash\n",
        "     python -m data_ingest.omni_cli file data/raw/users.csv --redact-pii --save\n",
        "     ```\n",
        "\n",
        "   - You can edit this line if your data source is different (e.g. `\"sql \u2026\"` or `\"rest \u2026\"`).\n",
        "\n",
        "2. **Phase\u00a03 \u2014 Data Preparation**\n",
        "\n",
        "   - Runs\n",
        "\n",
        "     ```bash\n",
        "     python -m ml_pipeline.prepare --outlier iqr --scaler standard\n",
        "     ```\n",
        "\n",
        "   - Default choices: IQR outlier detection, StandardScaler, median/mode imputation, no balancing.\n",
        "   - Checks that `data/interim/clean.parquet` and `data/processed/scaled.parquet` are produced.\n",
        "\n",
        "3. **Phase\u00a04 \u2014 EDA (Core)**\n",
        "\n",
        "   - Runs\n",
        "\n",
        "     ```bash\n",
        "     python -m Data_Analysis.EDA --mode all --target is_churn --profile\n",
        "     ```\n",
        "\n",
        "   - Generates univariate, bivariate, multivariate stats + plots, and an optional HTML profile.\n",
        "\n",
        "4. **Phase\u00a04D \u2014 EDA (Advanced)**\n",
        "\n",
        "   - Runs\n",
        "\n",
        "     ```bash\n",
        "     python -m Data_Analysis.EDA_advance\n",
        "     ```\n",
        "\n",
        "   - Provides deeper analyses (mutual information, leakage sniff, TS decor, etc.).\n",
        "\n",
        "5. **Phase\u00a04\u00bd \u2014 Probabilistic Analysis**\n",
        "\n",
        "   - Runs\n",
        "\n",
        "     ```bash\n",
        "     python -m data_analysis.probabilistic_analysis --impute_method mice --target is_churn\n",
        "     ```\n",
        "\n",
        "   - You can add `--do_pit`, `--do_quantile`, or `--do_copula` if desired.\n",
        "\n",
        "6. **Phase\u00a04\u00bd \u2014 Feature Selection**\n",
        "\n",
        "   - Runs\n",
        "\n",
        "     ```bash\n",
        "     python -m Feature_Selection.feature_select --data data/processed/scaled.parquet --target is_churn --nzv_threshold 1e-5 --corr_threshold 0.95 --mi_quantile 0.10\n",
        "     ```\n",
        "\n",
        "   - Outputs `data/processed/selected.parquet` and `reports/feature/feature_audit.json`.\n",
        "\n",
        "7. **Phase\u00a05 \u2014 Feature Engineering**\n",
        "\n",
        "   - Runs\n",
        "\n",
        "     ```bash\n",
        "     python -m Feature_Engineering.feature_engineering \\\n",
        "       --data data/processed/selected.parquet \\\n",
        "       --target is_churn \\\n",
        "       --numeric_scaler robust \\\n",
        "       --numeric_power yeo \\\n",
        "       --log_cols revenue \\\n",
        "       --quantile_bins age:4 \\\n",
        "       --polynomial_degree 2 \\\n",
        "       --rare_threshold 0.01 \\\n",
        "       --cat_encoding target \\\n",
        "       --text_vectorizer tfidf \\\n",
        "       --text_cols review \\\n",
        "       --datetime_cols last_login \\\n",
        "       --cyclical_cols hour:24 \\\n",
        "       --date_delta_cols signup_date:today \\\n",
        "       --aggregations customer_id:amount_mean,amount_sum \\\n",
        "       --drop_nzv \\\n",
        "       --corr_threshold 0.95 \\\n",
        "       --mi_quantile 0.10\n",
        "     ```\n",
        "\n",
        "   - Outputs `models/preprocessor.joblib`, `models/preprocessor.json` (SHA), plus\n",
        "     `reports/feature/feature_audit.json` and `reports/feature/feature_shape.txt`.\n",
        "\n",
        "8. **Phase\u00a05\u00bd \u2014 Split & Baseline**\n",
        "\n",
        "   - Runs\n",
        "\n",
        "     ```bash\n",
        "     python -m Data_Cleaning.split_and_baseline --target is_churn --seed 42 --stratify\n",
        "     ```\n",
        "\n",
        "   - Creates `data/splits/{train.parquet,\u00a0val.parquet,\u00a0test.parquet}`,\n",
        "     `data/splits/split_manifest.json`, and `reports/baseline/baseline_metrics.json`,\n",
        "     and snapshots `models/preprocessor_manifest.json`.\n",
        "\n",
        "9. **Phase\u00a06 \u2014 Train + Tune**\n",
        "\n",
        "   - Runs\n",
        "\n",
        "     ```bash\n",
        "     python -m model.train\n",
        "     ```\n",
        "\n",
        "   - Consumes splits and preprocessor, produces `models/model.pkl`, `models/model_card.md`, and training metrics in `reports/metrics/`.\n",
        "\n",
        "10. **Phase\u00a07 \u2014 Evaluate**\n",
        "\n",
        "    - Runs\n",
        "\n",
        "      ```bash\n",
        "      python -m model.evaluate\n",
        "      ```\n",
        "\n",
        "    - Consumes `data/splits/test.parquet` and `models/model.pkl`;\n",
        "      produces `reports/metrics/test_metrics.json` and `reports/metrics/roc_curve.csv`.\n",
        "\n",
        "11. **Phase\u00a08 \u2014 Package \u2192 ONNX**\n",
        "\n",
        "    - Runs\n",
        "\n",
        "      ```bash\n",
        "      python -m model.package --model models/model.pkl\n",
        "      ```\n",
        "\n",
        "    - Exports `artefacts/model.onnx` (if implemented).\n",
        "\n",
        "12. **Phase\u00a09 \u2014 Deploy (Optional)**\n",
        "\n",
        "    - If `deploy/push_to_registry.sh` exists, runs it. Otherwise, logs \u201cSKIP\u201d.\n",
        "\n",
        "## How to Use\n",
        "\n",
        "1. **Dry\u2011Run (Data Diagnostics + EDA + Probabilistic + Feature Selection)**\n",
        "   This mode will _only_ run diagnostics and analysis on your existing interim dataset (`data/interim/clean.parquet`) and then exit.\n",
        "\n",
        "   ```bash\n",
        "   python run_pipeline.py --dry-run\n",
        "   ```\n",
        "\n",
        "   - **Data Diagnostics** (missing values, imbalance, skewness, outliers)\n",
        "   - **Core EDA** \u2192 `python -m Data_Analysis.EDA --mode all --target is_churn`\n",
        "   - **Advanced EDA** \u2192 `python -m Data_Analysis.EDA_advance`\n",
        "   - **Probabilistic Analysis** \u2192 `python -m data_analysis.probabilistic_analysis`\n",
        "   - **Feature Selection** \u2192 `python -m Feature_Selection.feature_select`\n",
        "\n",
        "   > After these steps, the script prints a message and exits without running the rest of the pipeline.\n",
        "\n",
        "2. **Full Pipeline (End\u2011to\u2011End)**\n",
        "   This mode executes every phase in sequence:\n",
        "\n",
        "   1. **Phase\u00a02 \u2013 Data Collection**\n",
        "\n",
        "      ```bash\n",
        "      python -m data_ingest.omni_cli file data/raw/users.csv --redact-pii --save\n",
        "      ```\n",
        "\n",
        "   2. **Phase\u00a03 \u2013 Data Preparation**\n",
        "\n",
        "      ```bash\n",
        "      python -m ml_pipeline.prepare --outlier iqr --scaler standard --target is_churn\n",
        "      ```\n",
        "\n",
        "      (Add `--knn` or `--balance smote` if you modify `PREP_DEFAULT_ARGS` to `True`.)\n",
        "\n",
        "   3. **Phase\u00a04 \u2013 Core EDA**\n",
        "\n",
        "      ```bash\n",
        "      python -m Data_Analysis.EDA --mode all --target is_churn\n",
        "      ```\n",
        "\n",
        "   4. **Phase\u00a04D \u2013 Advanced EDA**\n",
        "\n",
        "      ```bash\n",
        "      python -m Data_Analysis.EDA_advance\n",
        "      ```\n",
        "\n",
        "   5. **Phase\u00a04\u00bd \u2013 Probabilistic Analysis**\n",
        "\n",
        "      ```bash\n",
        "      python -m data_analysis.probabilistic_analysis\n",
        "      ```\n",
        "\n",
        "   6. **Phase\u00a04\u00bd \u2013 Feature Selection**\n",
        "\n",
        "      ```bash\n",
        "      python -m Feature_Selection.feature_select --nzv_threshold 1e-5 --corr_threshold 0.95 --mi_quantile 0.1\n",
        "      ```\n",
        "\n",
        "   7. **Phase\u00a05 \u2013 Feature Engineering**\n",
        "\n",
        "      ```bash\n",
        "      python -m Feature_Engineering.feature_engineering \\\n",
        "        --data data/processed/selected.parquet \\\n",
        "        --target is_churn \\\n",
        "        --numeric_scaler robust \\\n",
        "        --numeric_power yeo \\\n",
        "        --log_cols revenue \\\n",
        "        --quantile_bins age:4 \\\n",
        "        --polynomial_degree 2 \\\n",
        "        --rare_threshold 0.01 \\\n",
        "        --cat_encoding target \\\n",
        "        --text_vectorizer tfidf \\\n",
        "        --text_cols review \\\n",
        "        --datetime_cols last_login \\\n",
        "        --cyclical_cols hour:24 \\\n",
        "        --date_delta_cols signup_date:2023-01-01 \\\n",
        "        --aggregations customer_id:amount_mean,amount_sum \\\n",
        "        --drop_nzv \\\n",
        "        --corr_threshold 0.95 \\\n",
        "        --mi_quantile 0.1\n",
        "      ```\n",
        "\n",
        "   8. **Phase\u00a05\u00bd \u2013 Split & Baseline**\n",
        "\n",
        "      ```bash\n",
        "      python -m Data_Cleaning.split_and_baseline --target is_churn --seed 42 --stratify\n",
        "      ```\n",
        "\n",
        "   9. **Phase\u00a06 \u2013 Model Training & Tuning**\n",
        "\n",
        "      ```bash\n",
        "      python -m model.train\n",
        "      ```\n",
        "\n",
        "      (Add any flags by editing `TRAIN_DEFAULT_ARGS` at top of this script.)\n",
        "\n",
        "3. **Phase\u00a07 \u2013 Evaluation**\n",
        "\n",
        "   ```bash\n",
        "   python -m model.evaluate\n",
        "   ```\n",
        "\n",
        "4. **Phase\u00a08 \u2013 Packaging**\n",
        "\n",
        "   ```bash\n",
        "   python -m model.package\n",
        "   ```\n",
        "\n",
        "5. **Phase\u00a09 \u2013 Deployment**\n",
        "\n",
        "   ```bash\n",
        "   bash deploy/push_to_registry.sh\n",
        "   ```\n",
        "\n",
        "> Because **`run_pipeline.py`** no longer references `params.yaml`, you can either edit the hard\u2011coded defaults at the very top of `run_pipeline.py` (e.g. change `TARGET_COLUMN`, adjust `OMNI_CLI_DEFAULT_ARGS`, switch to SMOTE, etc.) or simply let it run with those values as\u2011is.\n",
        "\n",
        "---\n",
        "\n",
        "### Summary of Key Points\n",
        "\n",
        "- **No `params.yaml` dependency**\u2014all defaults live in `run_pipeline.py`.\n",
        "- **Dry\u2011Run mode** (`--dry-run`)\n",
        "  \u2022 Loads `data/interim/clean.parquet` \u2192 runs data diagnostics (missingness, imbalance, skew, outliers).\n",
        "  \u2022 Runs core EDA + advanced EDA via Python modules (no need for manual notebook).\n",
        "  \u2022 Runs probabilistic analysis.\n",
        "  \u2022 Runs feature selection.\n",
        "  \u2022 Then exits.\n",
        "- **Full Pipeline** (no flags)\n",
        "  \u2022 Invokes each phase in order via shell commands\u2014data ingestion \u2192 prep \u2192 EDA \u2192 prob analysis \u2192 feature selection \u2192 feature engineering \u2192 split & baseline \u2192 train \u2192 evaluate \u2192 package \u2192 deploy.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run `ML_Flow.py`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%run \"pipeline/ML_Flow.py\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run `ML_pipeline.py`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%run \"pipeline/ML_pipeline.py\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run `ml_3.py`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%run \"pipeline/ml_3.py\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run `ml_flow2.py`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%run \"pipeline/ml_flow2.py\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run `run_pipeline.py`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%run \"pipeline/run_pipeline.py\""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}