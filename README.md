# ğŸ‘¾ MLDLC â€“ End-to-End Machine-Learning Project Template

This repository is a **ready-made workflow skeleton** for any ML project, small
or large.  
It captures the full _Machine-Learning Development Life-Cycle_ (MLDLC) in
clearly separated phasesâ€”so every new problem you tackle starts with the same,
battle-tested structure instead of an empty folder.

## Why use this repo?

- **Consistent anatomy** â€“ one place for data, code, reports, and models
- **Phase gates** â€“ checklists ensure you donâ€™t jump ahead with fuzzy scope
- **Security & governance hooks** â€“ PII masking, lineage, and basic compliance
- **Extensible** â€“ each phase ships with a runnable Python stub you can swap or extend
- **Tool-agnostic** â€“ works for tabular, NLP, vision, or time-series; local or cloud

## Repo Scaffold

```text
.
â”œâ”€â”€ data/                         # raw/, interim/, processed/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ Data Ingestion/
â”‚   â”‚   â””â”€â”€ data_collector.py     # Phase-2 engine  (â‡¨ see link below)
â”‚   â”œâ”€â”€ Data Cleaning/
â”‚   â”‚   â”œâ”€â”€ data_preparation.py   # Prep pipeline  (Phase-3)
â”‚   â”‚   â””â”€â”€ data_wrangling_cleaning.py
â”‚   â”œâ”€â”€ Data Analysis/
â”‚   â”‚   â”œâ”€â”€ EDA.py                # Basic EDA      (Phase-4)
â”‚   â”‚   â””â”€â”€ EDA_advance.py        # Advanced EDA   (Phase-4 extra)
â”‚   â””â”€â”€ Feature Engineering/
â”‚       â””â”€â”€ feature_engineering.py
â”œâ”€â”€ notebooks/                    # Optional ad-hoc ipynb
â”œâ”€â”€ reports/                      # Auto-generated EDA, drift, model cards
â”œâ”€â”€ models/                       # MLflow or on-disk artefacts
â”œâ”€â”€ docker/                       # Dockerfile & helpers
â”œâ”€â”€ dvc.yaml                      # DVC pipeline
â”œâ”€â”€ pyproject.toml                # editable-install metadata
â”œâ”€â”€ .github/                      # CI/CD workflows
â””â”€â”€ README.md                     # â† youâ€™re reading it
```

---

## Table of Contents

0. [Repo Scaffold](#0-repo-scaffold)

1. [Phase 1 â€” Problem Definition](#1-phase-1--problem-definition)

2. [Phase 2 â€” **Data Collection**](src/Data%20Ingestion/omni_collector.py)
   â€¢ [2A Flat-Files & Object Storage](#2a-flat-files--object-storage)
   â€¢ [2B Relational Databases](#2b-relational-databases)
   â€¢ [2C NoSQL & Analytical Stores](#2c-nosql--analytical-stores)
   â€¢ [2D APIs & Web Scraping](#2d-apis--web-scraping)
   â€¢ [2E Streaming & Message Queues](#2e-streaming--message-queues)
   â€¢ [2F SaaS & Cloud-Native Connectors](#2f-saas--cloud-native-connectors)
   â€¢ [2G Sensors & IoT](#2g-sensors--iot)
   â€¢ [2H Data Privacy & Governance Hooks](#2h-data-privacy--governance-hooks)
   â€¢ [2I Logging, Auditing & Checksums](#2i-logging-auditing--checksums)

3. [Phase 3 â€” **Data Preparation**](src/Data%20Cleaning/data_preparation.py)
   â€¢ [3A Schema Validation & Data Types](#3a-schema-validation--data-types)
   â€¢ [3B1 De-duplication and Invariant Pruning](#3b-dedup)
   â€¢ [3B Missing-Value Strategy](#3b-missing-value-strategy)
   â€¢ [3C Outlier Detection & Treatment](#3c-outlier-detection--treatment)
   â€¢ [3D Data Transformation & Scaling](#3d-data-transformation--scaling)
   â€¢ [3E Class / Target Balancing](#3e-class-target-balancing)
   â€¢ [3F Data Versioning & Lineage](#3f-data-versioning--lineage)
   â€¢ [3G Feature Pruning (High NaN / High Corr)](#3g-prune)
4. [Phase 4 â€” **Exploratory Data Analysis (EDA)**](src/Data%20Analysis/EDA.py)
   â€¢ [4A Univariate Statistics & Plots](#4a-univariate-statistics--plots)
   â€¢ [4B Bivariate Tests & Visuals](#4b-bivariate-tests--visuals)
   â€¢ [4C Multivariate Tests & Diagnostics](#4c-multivariate-tests--diagnostics)
   â€¢ [4D Advanced EDA (MI Â· Cramer-V Â· Embeddings Â· TS Decomp)](src/Data%20Analysis/EDA_advance.py)

5Â·Â½. [Dataset Partition & Baseline Benchmarking](#5.5-phase-baseline-freeze)

5. [Phase 5 â€” Feature Engineering](src/Feature%20Engineering/feature_engineering.py)
   â€¢ [5A Scaling & Normalization](#5-phase-5--feature-engineering)
   â€¢ [5B Encoding Categorical Variables](#5b-encoding-categorical-variables)
   â€¢ [5C Handling Imbalanced Data](#5c-handling-imbalanced-data)
   â€¢ [5D Dimensionality Reduction](#5d-dimensionality-reduction)
   â€¢ [5E Custom Feature-Engineering Plug-ins](#5e-custom--advanced-plug-ins)

6. [Phase 6 â€” Model Design & Training](#6-phase-6--model-design--training)
   â€¢ [6A Algorithm Selection](#6a-algorithm-selection)
   â€¢ [6B Regularisation Techniques](#6b-regularisation-techniques)
   â€¢ [6C Cross-Validation Variants](#6c-cross-validation-variants)
   â€¢ [6D Hyper-Parameter Optimisation](#6d-hyper-parameter-optimisation)
   â€¢ [6E Early-Stopping & LR Scheduling](#6e-early-stopping--lr-scheduling)
   â€¢ [6F Ensembling & Bagging / Stacking](#6f-ensembling--bagging--stacking)
   â€¢ [6G Data Augmentation & Noise Injection](#6g-data-augmentation)

7. [Phase 7 â€” **Evaluation, Regularisation Audit & Hardening**](#7-phase-7--evaluation-regularisation--hardening)
   â€¢ [7A Core Metrics](#7a-core-metrics)
   â€¢ [7B Calibration & Probabilistic Quality](#7b-calibration--probability-quality)
   â€¢ [7C Bias / Fairness & Group Metrics](#7c-bias--fairness)
   â€¢ [7D Explainability (SHAP Â· LIME Â· XAI)](#7d-explainability)
   â€¢ [7E Robustness & Adversarial Testing](#7e-robustness--adversarial-testing)
   â€¢ [7F Over-fitting Diagnostics](#7f-over-fitting-diagnostics)
   â€¢ [7G Model Card & Governance Sign-off](#7g-model-card--governance)

8. [Phase 8 â€” **Deployment & Serving**](#8-phase-8--deployment--serving)
   â€¢ [8A Model Serialization](#8a-model-serialization)
   â€¢ [8B Packaging & Containerization](#8b-packaging--containerization)
   â€¢ [8C API & Micro-service Layer](#8c-api--micro-service-layer)
   â€¢ [8D Inference Optimisation](#8d-inference-optimisation)
   â€¢ [8E CI/CD & Model-Registry Promotion](#8e-cicd--model-registry-promotion)
   â€¢ [8F Release Strategies](#8f-release-strategies)
   â€¢ [8G Runtime Security](#8g-runtime-security)

9. [Phase 9 â€” **Monitoring, Drift & Retraining**](#9-phase-9--monitoring-drift--retraining)
   â€¢ [9A Performance & Latency Metrics](#9a-performance--latency-metrics)
   â€¢ [9B Data & Concept Drift Detection](#9b-data--concept-drift-detection)
   â€¢ [9C Model Quality Tracking & Alerts](#9c-model-quality-tracking--alerts)
   â€¢ [9D Logging & Audit Trails](#9d-logging--audit-trails)
   â€¢ [9E Automated Retraining Pipelines](#9e-automated-retraining-pipelines)
   â€¢ [9F Rollback / Roll-forward Playbooks](#9f-rollback--roll-forward-playbooks)
   â€¢ [9G Continuous Compliance & Model Registry](#9g-continuous-compliance--model-registry)

10. [Cloud-Security Pillars](#10-cloud-security-pillars)

11. [CI/CD & Automation](#11-cicd--automation)

12. [FAQ](#12-faq)

13. [License](#13-license)

## 1 â€” Phase 1 Â· Problem Definition<a name="1-phase-1--problem-definition"></a>

> **Goal** â€” crystallise a fuzzy idea into an _implementable, testable and
> measurable_ ML plan.  
> This phase is finished only when every item in the **Exit Checklist** is
> tick-boxed and signed off.

---

### 1 Â· Clarify the Business â€œWhyâ€

| Ask                                                     | Why it matters                                         |
| ------------------------------------------------------- | ------------------------------------------------------ |
| _â€œIf the model is perfect tomorrow, what changes?â€_     | forces ROI thinking, surfaces hidden KPIs              |
| _â€œWho loses sleep if this fails?â€_                      | reveals actual decision-makers / veto-holders          |
| _â€œWhat is the cost of a wrong prediction?â€_             | calibrates class-imbalance weighting, thresholds, SLAs |
| **â€œWhat is the current manual / heuristic benchmark?â€** | Defines the _baseline to beat_                         |

---

### 2 Â· Translate to an ML Task

1. **Prediction vs. ranking vs. clustering?**
   Map to _supervised_, _recommender_, _unsupervised_ or _forecasting_ bucket.
2. **Unit of prediction** (row-level? session? account? pixel?).
   Mis-scoping here kills performance later.
3. **Latency tolerance** â†’ batch, near-real-time, or streaming.
4. **Interpretability & constraints** &nbsp;â€”&nbsp; e.g. model must be explainable
5. **Regulatory context** &nbsp;â€”&nbsp; GDPR, HIPAA, sector-specific rules

> _Tip_: if you canâ€™t phrase the target as a column in a future CSV,
> you donâ€™t have a learnable task yet.

---

### 3 Â· Do a Data Reality Check _before_ Deep EDA

- Column availability at **prediction time** (no future leakage).
- Gauge **volume vs freshness vs drift risk**.
- Quick uni-variate histograms â†’ smell test for PII, bogus zeros, unit errors.
- Compare simple **baselines** (constant, majority-class, heuristic) to KPI.
  If a baseline already beats the target KPI, challenge the need for ML.

---

### 4 Â· Scope & Deliverables (one-pager)

| Section          | Example entry (fill in)                                      |
| ---------------- | ------------------------------------------------------------ |
| **In-scope**     | Web-app customers, last 2 years of activity logs             |
| **Out-of-scope** | Mobile-app only users, customer-support notes                |
| **Deliverables** | â‘  Trained model artifact<br>â‘¡ Inference API spec<br>â‘¢ README |
| **Timeline**     | 12 weeks (M1-M3), pilot rollout in Q4                        |

---

### 5 Â· SMART Success Criteria & Metrics

| Category         | Target metric & threshold | Why this metric?                             |
| ---------------- | ------------------------- | -------------------------------------------- |
| **Primary**      | ROC-AUC â‰¥ 0.85            | Class-imbalance; need good ranking power     |
| **Secondary**    | F1 â‰¥ 0.75                 | Balance precision & recall for interventions |
| **Business KPI** | â†“ churn rate by 15 %      | Direct financial impact                      |

> **Justify the metric**: ROC-AUC is threshold-independent and robust for
> imbalanced data; F1 captures the harmonic mean of precision & recall which
> suits the â€œfind true churnersâ€ objective.

---

### 6 Â· Baseline Expectation

> Current heuristic = _always predict â€œno churnâ€_  
> â†’ Accuracy â‰ˆ 75 %, F1 â‰ˆ 0.00, ROC-AUC â‰ˆ 0.50  
> Our ML model **must beat this baseline** on the hold-out set to be worthwhile.

---

### 7 Â· Sketch the End-to-End Flow on One Whiteboard

```mermaid
flowchart LR
    subgraph Offline
        A[Raw Sources] --> B[Data Prep<br>+ EDA] --> C[Train / Tune]
    end
    subgraph Online
        C --> D(Model Artifact) --> E[API / Batch Job]
    end
    E -->|logs| F[Monitoring & Drift]
    F -->|trigger| B
```

---

### 8 Problem-Clarity âœ… **Exit Checklist**

_(all boxes must be ticked before Phase 2 â€“ Data Collection â€“ may start)_

- [ ] **Business objective** phrased as one SMART sentence  
       _e.g._ â€œReduce voluntary churn by 15 % within 2 quartersâ€
- [ ] **Unit of analysis** defined â€“ â€œprediction per _customer-ID_ per monthâ€
- [ ] **Target variable** unambiguously stated and time-stamped  
       (`is_churn` âˆˆ {0, 1} measured 30 days after billing date)
- [ ] **Primary success metric** & numeric threshold agreed â€“ â€œF1 â‰¥ 0.82 on Q4 hold-outâ€
- [ ] **Constraints & assumptions** captured (latency, region, budget, feature-freeze date)
- [ ] **High-level ethical / bias risks** listed (sensitive attributes, exclusion harms)
- [ ] **Baseline approach** written down (random or simple heuristic score)
- [ ] **All items reviewed & signed off** (attach link in project tracker)

> When the table is fully checked, create an issue titled  
> **â€œPhase-1 Complete â€“ proceed to Data Collectionâ€** and assign it to the team lead.  
> Only then move on to Phase-2.

---

## 2 â€” Phase 2 Â· **Data Collection & Initial Validation**<a name="2-phase-2--data-collection"></a>

> **Goal** â€” pull tabular data from _any_ source, redact obvious PII,  
> version the raw snapshot in `data/raw/`, **and fail-fast** if the feed violates
> basic quality expectations.  
> Everything is orchestrated by **[`OmniCollector`](src/Data%20Ingestion/data_collector.py)**  
> (the â€œone ringâ€ in Phase-2).  
> When the collector finishes it hands a Parquet file to Phase-3 and writes an
> audit-trail line to `logs/ingest.log`.

### 2Â·0 What happens under the hood ğŸ› 

1. **Download / query / consume** via one of `OmniCollector.from_*` channels
2. **Regex PII scrub** (email & phone) â†’ _Data Privacy Hook_
3. **Great Expectations suite** runs (schema, non-null %, range checks, etc.)
   - Fails the pipeline if any _critical_ expectation is violated
4. **SHA-256 & row-count** logged to `logs/ingest.log`
5. Snapshot saved to `data/raw/â€¦` and git-/DVC-tracked

> Copy `great_expectations/expectations/sample_suite.yml` and tailor it to your
> dataset; the default checks only shape & non-null counts.

---

### 2A Flat-Files & Object Storage<a name="2a-flat-files--object-storage"></a>

| Format / Location         | Collector call                                    | Notes                                       |
| ------------------------- | ------------------------------------------------- | ------------------------------------------- |
| **CSV / TSV**             | `oc.from_file("data/raw/users.csv")`              | Auto-delimiter / Pandas dtype inference     |
| **Excel**                 | `oc.from_file("marketing.xlsx")`                  | Multi-sheet supported                       |
| **Parquet / ORC / Avro**  | `oc.from_file("events.parquet")`                  | Needs **pyarrow**                           |
| **S3 / GCS / Azure Blob** | `oc.from_file("s3://bucket/path/events.parquet")` | IAM role / KMS handled by `storage_options` |
| **ZIP / TAR**             | `oc.from_file("archive.zip")`                     | Auto-extract if single-member archive       |

_Governance_: set bucket-policy to SSE-KMS, use **least-privilege IAM**; the
collector runs regex-based email/phone redaction before snapshot-save.

---

### 2B Relational DBs<a name="2b-relational-databases"></a>

```python
dsn   = "postgresql+psycopg2://ml_user:${PG_PWD}@pg-ro.acme.local/warehouse"
query = """
SELECT uid, age, is_churn, updated_at
FROM   analytics.users
WHERE  updated_at >= CURRENT_DATE - INTERVAL '90 days'
"""
df = oc.from_sql(dsn, query)
```

_Parameterised queries_ avoid SQL-i, and read-only replicas protect prod.

---

### 2C NoSQL / Analytical Stores<a name="2c-nosql--analytical-stores"></a>

```python
df = oc.from_mongo(
        uri="mongodb://ro_user:${MONGO_PWD}@mongo-ro:27017",
        db="crm", coll="events",
        query={"updated_at": {"$gte": "2025-01-01"}}
)
```

(BigQuery & Snowflake use `from_sql` via SQLAlchemy drivers.)

---

### 2D APIs & Web Scraping<a name="2d-apis--web-scraping"></a>

```python
df_fx = oc.from_rest(
          "https://api.exchangerate.host/latest",
          params={"base": "USD"}
)
```

Scraping? Use `BeautifulSoup` or Playwright, then `oc.save(df, "source_name")`.

If you must scrape:

```python
from bs4 import BeautifulSoup, requests
html = requests.get("https://example.com/pricelist", timeout=15).text
price_df = pd.read_html(str(BeautifulSoup(html,"lxml").find("table")))[0]
oc.save(price_df, "price_table")
```

_Security_: respect robots.txt, user-agent throttling, rotate tokens.

### 2E Streams / Message Queues<a name="2e-streaming--message-queues"></a>

```python
stream_df = oc.from_kafka(
               topic="tx-events",
               bootstrap="broker:9092",
               batch=1_000,
               group_id="ingest-probe"
)
```

Offsets committed **after** `oc.save()` âœ at-least-once semantics.

---

### 2F SaaS / Cloud-Native<a name="2f-saas--cloud-native-connectors"></a>

```python
df_sheet = oc.from_gsheet(
              sheet_key=os.getenv("GSHEET_ID"),
              creds_json="gcp-sa.json"
)
```

Need HubSpot, Stripe, Salesforce?
Either:

1. Call their REST/Bulk API â†’ `oc.from_rest()`, or
2. Use Fivetran / Airbyte to land data in Postgres/Snowflake, then `from_sql`.

---

### 2G Sensors & IoT<a name="2g-sensors--iot"></a>

```python
iot_df = oc.from_mqtt(
            broker="192.168.1.50",
            topic="factory/line1/#",
            timeout=10
)
```

Store raw telemetry uncompressed â†’ `Parquet+ZSTD` later in an Apache Iceberg or
TimescaleDB bucket for long-term analytics.

---

### 2H Data-Privacy & Governance Hooks<a name="2h-data-privacy--governance-hooks"></a>

- Regex redaction for emails & phone numbers (`_mask`)
- Extendable: plug your own `re` patterns or FPE tokenisers
- Lineage tags stored alongside the Parquet metadata (`dataset`, `source_system`, `run_id`)

---

### 2I Logging, Auditing & Checksums<a name="2i-logging-auditing--checksums"></a>

Every collector call:

1. **SHA-256** of the CSV bytes (or canonical Parquet bytes)
2. **row-count**
3. **source label**
4. UTC timestamp

Each ingest line in `logs/ingest.log`:

```
2025-05-30T22:14:09Z | INFO | flat:events.parquet | rows=104 876 | sha256=7b12e0f83e01
```

This + the DVC commit = tamper-evident audit trail.

---

### ğŸ”§ Quick-Start

```bash
# â‘  install core + sqlalchemy + great_expectations
pip install -e .[ingest,sql,ge]

# â‘¡ pull CSV snapshot
omni-collect file data/raw/users.csv

# â‘¢ check the log & Great Expectations report (html in great_expectations/â€¦)
```

---

### ğŸ“Œ Why the extra validation step?

1. **Fail-fast** â€“ bad schemas blow up here, not during model training
2. **Confidence for 50 k engineers** â€“ everyone inherits a baseline of QA
3. **CI-friendly** â€“ the GE suite runs in GitHub Actions so broken feeds block the PR

If you need stricter checks (e.g. â€œ< 5 % nulls in `age`â€), edit
`great_expectations/expectations/sample_suite.yml`.

---

> **Next phase âœ [Data Preparation](#3-phase-3--data-preparation)**
> You now have an immutable, validated snapshot ready for cleaning & scaling.

---

## 3 â€” Phase 3 Â· **Data Preparation**<a name="3-phase-3--data-preparation"></a>

> **Goal** â€” turn a raw snapshot from Phase-2 into a _model-ready_, versioned,
> privacy-hardened dataset in `data/processed/`, plus an interim copy in
> `data/interim/`.
> All logic lives in
> **[`src/ml_pipeline/prepare.py`](src/ml_pipeline/prepare.py)** â€”
> a configurable pipeline class (**`DataPreparer`**).

---

### 3A Schema Validation & Data Types<a name="3a-schema-validation--data-types"></a>

| Tool                        | What it does                                              | Where                                |
| --------------------------- | --------------------------------------------------------- | ------------------------------------ |
| **Pandera**                 | enforce column names, dtypes, value ranges, allowed enums | `schema = pa.DataFrameSchema({...})` |
| **pyjanitor**               | snake-cases column names (`df.clean_names()`)             | first line of `load_and_validate()`  |
| Data-quality tests (opt-in) | `great_expectations` (`--gx`)                             | `dq_validate()`                      |

**Why:** catch bad upstream changes early; guarantee downstream code never
breaks on dtype surprises.

---

### 3B.1 De-duplication & Invariant Pruning <a name="3b1-dedup"></a>

- `--dedup uid` â†’ drops perfect-duplicate _rows_.
- `--prune-const 0.99` â†’ removes columns where one value â‰¥ 99 %.

---

### 3B Missing-Value Strategy<a name="3b-missing-value-strategy"></a>

_Default_: median (numeric) + mode (categorical).
_Optional_: `--knn` flag enables **`KNNImputer`** (k=5).

| Technique      | Flag              | Notes                                |
| -------------- | ----------------- | ------------------------------------ |
| Median / Mode  | _(default)_       | fast & deterministic                 |
| **KNNImputer** | `--knn`           | non-linear numeric guess             |
| Drop column    | `--drop-miss 0.4` | removes any feature with > 40 % NaNs |
| Drop row       | `--drop-miss 0.4` | removes any row with > 40 % NaNs     |

```bash
python -m ml_pipeline.prepare --knn      # fancy impute
```

_Diagnostics:_ generates a `missingno.matrix` plot for the first 1 000 rows.

---

### 3C Outlier Detection & Treatment<a name="3c-outlier-detection--treatment"></a>

| Method             | Flag                      | Notes                          |                    |                            |
| ------------------ | ------------------------- | ------------------------------ | ------------------ | -------------------------- |
| IQR fence (1.5Ã—)   | `--outlier iqr` (default) | quick & interpretable          |                    |                            |
| Z-score (          | z                         | < 3)                           | `--outlier zscore` | good for gaussian-ish data |
| Isolation Forest   | `--outlier iso`           | detects multivariate anomalies |                    |                            |
| Local Outlier Fac. | `--outlier lof`           | cluster-shaped data            |

---

### 3D Data Transformation & Scaling<a name="3d-data-transformation--scaling"></a>

| Transform                          | Flag                       | Comment                    |
| ---------------------------------- | -------------------------- | -------------------------- |
| log-transform on `amount`          | on by default (`np.log1p`) | stabilise heavy-tail       |
| **StandardScaler**                 | `--scaler standard`        | zero-mean / unit-var       |
| **RobustScaler** (IQR)             | `--scaler robust`          | heavy-outlier datasets     |
| **PowerTransformer (Yeo-Johnson)** | `--scaler yeo`             | make data closer to normal |

---

### 3E Class / Target Balancing<a name="3e-class-target-balancing"></a>

| Technique                   | Flag                 | Use-case                  |
| --------------------------- | -------------------- | ------------------------- |
| **SMOTE** over-sampling     | `--balance smote`    | minority boost            |
| **NearMiss** under-sampling | `--balance nearmiss` | huge majority down-sample |

```bash
python -m ml_pipeline.prepare --balance smote
```

---

### 3F Data Versioning & Lineage<a name="3f-data-versioning--lineage"></a>

- Saves **both** `data/interim/clean.parquet` (pre-scale) _and_
  `data/processed/scaled.parquet` (final).
- Writes `reports/lineage/prep_manifest.json`, e.g.

```jsonc
{
  "timestamp": "2025-05-30T12:42:01",
  "rows": 104876,
  "scaler": "robust",
  "outlier": "iso",
  "balance": "smote",
  "raw_sha": "7b12e0f83e01"
}
```

Add these files to **DVC** or **LakeFS** so every model build can
pin-point exactly which prep config & raw snapshot produced it.

---

### 3G Feature Pruning (High NaN / High Corr) <a name="3g-prune"></a>

- **NaN threshold** `--drop-miss p` â†’ prune if NaNs > p
- **Corr threshold** `--drop-corr 0.95` â†’ greedily drop highly-correlated pair

Manifest of drops saved to `reports/lineage/prune_log.json`.

---

### ğŸ”§ Quick-Start Cheat-Sheet

```bash
# 1. Default happy-path (median/mode, IQR, standard scale)
python -m ml_pipeline.prepare

# 2. Robust pipeline for gnarly data
python -m ml_pipeline.prepare \
       --knn \
       --outlier iso \
       --scaler robust \
       --balance smote
```

## 4 â€” Phase 4 Â· **Exploratory Data Analysis (EDA)**<a name="4-phase-4--exploratory-data-analysis"></a>

> **Goal** â€” get a _holistic view_ of the dataset, its distributions, relationships,
> and potential issues.
> This phase is orchestrated by **[`EDA.py`](src/Data%20Analysis/EDA.py)**, which
> reads the pre-processed data from `data/interim/clean.parquet` (output of Phase-3)
> and writes all artefacts to `reports/eda/`.
>
> - **[`EDA.py`](src/Data Analysis/EDA.py)** â€“ univariate, bivariate, multivariate,
>   target-aware imbalance, leakage flags, optional HTML profile.
> - **[`EDA_advance.py`](src/Data Analysis/EDA_advance.py)** â€“ still available for
>   very heavy add-ons (UMAP, t-SNE, time-series seasonality, etc.).

Both scripts read `data/interim/clean.parquet` (output of Phase-3) and write to
`reports/eda/`.

> downstream notebooks (or model cards) can embed.

---

### 4A Univariate Statistics & Plots<a name="4a-univariate-statistics--plots"></a>

| Metric / Test                                                         | Implementation                     | Output artefact                                         |
| --------------------------------------------------------------------- | ---------------------------------- | ------------------------------------------------------- |
| mean, median, variance, std, skew, kurt                               | `df.amount.agg([...])`             | `reports/eda/univariate_summary.csv`                    |
| skew Â· kurt Â· IQR                                                     | `Series.skew()                     | kurt()`                                                 |
| Normality: Shapiroâ€“Wilk, Dâ€™Agostino KÂ², Jarqueâ€“Bera, Andersonâ€“Darling | `scipy.stats`                      | CSV columns `shapiro_p`, `dagostino_p`, `jb_p`          |
| Normality p-values                                                    | Shapiro, Dâ€™Agostino, Jarqueâ€“Bera   | columns `shapiro_p`, `dagostino_p`, `jb_p`              |
| Visuals                                                               | Histogram + KDE, box-plot, QQ-plot | one PNG per numeric feature in `reports/eda/uva/plots/` |

> **Run only this section**
>
> ```bash
> python -m Data_Analysis.EDA --mode uva
> ```

---

### 4B Bivariate Tests & Visuals<a name="4b-bivariate-tests--visuals"></a>

| Pair Type        | Parametric                         | Non-Parametric        | Effect-size      |
| ---------------- | ---------------------------------- | --------------------- | ---------------- |
| num-num          | Pearson r                          | Spearman Ï, Kendall Ï„ | `rÂ²`,joint-plots |
| num vs 2 groups  | Welch-t                            | Mannâ€“Whitney U        | Cohenâ€™s d        |
| num vs k groups  | ANOVA                              | Kruskalâ€“Wallis        | Î·Â²               |
| cat-cat          | Ï‡Â²                                 | Fisher exact (2Ã—2)    | Cramer V         |
| num â†” num        | Pearson r Â· Spearman Ï Â· Kendall Ï„ | optional              |
| num â†” binary tgt | Point-Biserial r                   | effect-size in CSV    |
| num â†” multi tgt  | Pearson r                          |

- **Joint-plot regressions** and **correlation heat-map** saved to  
  `reports/eda/bva/plots/`.
- Results table â†’ `bivariate_summary.csv`.

Correlation heat-map & individual regressions are generated only when
`--pairplots` is passed.

```bash
python -m Data_Analysis.EDA --mode bva --target is_churn --pairplots
```

---

### 4C Multivariate Tests & Diagnostics<a name="4c-multivariate-tests--diagnostics"></a>

| Goal                   | Test / Tool                     | File / Visual                 |
| ---------------------- | ------------------------------- | ----------------------------- |
| Multi-collinearity     | max **VIF** across features     | `vif.csv`, `mva_summary.json` |
| Multivariate normality | **Mardia** P-val < 0.05         | `mva_summary.json`            |
| Overall association    | MANOVA (Pillaiâ€™s Trace)         | printed to console            |
| Dimensionality         | PCA scree â‰¥ 90 %                | `pca_scree.png`               |
| Cluster tendency       | **Hopkins** statistic           | `mva_summary.json`            |
| Heteroscedasticity     | **Breuschâ€“Pagan** p-value       | `mva_summary.json`            |
| Correlation dendrogram | seaborn `clustermap`            | `corr_dendrogram.png`         |
| Leakage guard          | AUC â‰ˆ 1 features â†’ flagged JSON | `leakage_flags.json`          |

- **VIF**: Variance Inflation Factor, max VIF > 10 â†’ multicollinearity
- **Mardia**: tests multivariate normality; p-value < 0.05 â†’ reject H0
- **Hopkins**: tests cluster tendency; H0 = uniform distribution, H1 = clustering
- **Breuschâ€“Pagan**: tests heteroscedasticity; p-value < 0.05 â†’ reject H0
- **Dendrogram**: visualizes correlation structure; clusters of features
- **Leakage guard**: checks for future-timestamp overlap; flags features with AUC â‰ˆ 1
- **PCA scree**: plots explained variance by components; helps decide dimensionality
- **MANOVA**: multivariate analysis of variance; checks if group means differ significantly
- **Pair-plots**: scatter matrix of numeric features, colored by target class

```bash
python -m Data_Analysis.EDA --mode mva --target is_churn
```

---

### 4D Advanced EDA â€” Mutual Info Â· Cramer-V Â· Embeddings Â· TS Decomp<a name="src/Data%20Analysis/EDA_advance.py"></a>

File: **[`EDA_advance.py`](src/Data%20Analysis/EDA_advance.py)**

What it adds on top of 4A-4C:

| Block                   | Highlight                                   |
| ----------------------- | ------------------------------------------- |
| Categorical association | **Cramer-V matrix** + mosaic plots          |
| Feature importance      | **Mutual Information** (numeric & one-hot)  |
| Interaction viz         | PairGrid by target, 2-D UMAP / 3-D t-SNE    |
| Leakage sniff           | Future-timestamp overlap check              |
| Time-series             | Seasonal decomposition, ACF/PACF plots      |
| Clustering quality      | k-means **elbow** + **silhouette** curves   |
| Auto-profilers          | `ydata_profiling` HTML, `dabl.plot` summary |

Outputs land in `reports/eda/advanced/`:

```bash
python -m Data_Analysis.EDA_advance
```

---

#### ğŸ” Where to look after a run

```
reports/
â””â”€â”€ eda/
    â”œâ”€â”€ univariate_summary.csv
    â”œâ”€â”€ bivariate_summary.csv
    â”œâ”€â”€ vif.csv
    â”œâ”€â”€ mva_summary.json
    â”œâ”€â”€ uva/plots/*.png
    â”œâ”€â”€ bva/plots/*.png
    â”œâ”€â”€ mva/plots/*.png
    â””â”€â”€ advanced/
        â”œâ”€â”€ mutual_info.csv
        â”œâ”€â”€ profile.html
        â””â”€â”€ *.png
```

---

### ğŸ›  CLI Cheat-Sheet

```bash
# lightweight (stats only)
python -m Data_Analysis.EDA --target is_churn

# full deep-dive with pair-plots + HTML profile
python -m Data_Analysis.EDA \
       --target is_churn \
       --pairplots \
       --profile
```

---

## 4Â·Â½. [Feature Selection & Early Train/Test Split](#4.5-phase-feature-select-split)

> **Why here?** Any statistic that _uses_ the target (variance filter,  
> mutual-information, Cramer-V, leakage sniff, etc.) must be learned on
> **training rows only**.  
> Therefore we:
>
> 1. **Split once â€” right now** (80 / 20 stratified by `target`  
>    or `--time-split` if temporal).
> 2. **Fit feature filters on _train_**, replay them on _val_ / _test_.
>    | Sub-step | Purpose | Script | Artefact |
>    | --------------------------- | ------------------------------------- | --------------------- | --------------------------------------------- |
>    | **4Â·Â½Â·0 Split** | Freeze leak-free `train / val / test` | `feature_selector.py` | `data/splits/*.parquet` `split_manifest.json` |
>    | **4Â·Â½Â·1 Low-variance drop** | remove near-constant cols | â€³ | logged in manifest |
>    | **4Â·Â½Â·2 Target filter** | MI / chiÂ² < threshold | â€³ | `"kept","dropped"` lists |
>    | **4Â·Â½Â·3 Collinearity** | drop one of pairs with Ï > 0.95 | â€³ | correlation heatmap |
>    | **4Â·Â½Â·4 Save plan** | Column lists for next phases | `"feature_plan.json"` |

```bash
 # full run â€“ stratified split, MI filter @ 0.001, corr prune @ 0.95
 python -m Data_Analysis.feature_selector \
    --target is_churn \
    --mi-thresh 0.001 \
    --corr-thresh 0.95 \
    --seed 42
```

**Exit checklist** _ âœ… `data/splits/train.parquet` & `test.parquet` exist  
 _ âœ… `feature_plan.json` lists â€œkeepâ€ & â€œdropâ€ columns  
 _ âœ… No feature on the **drop list** is referenced downstream  
 _ âœ… Issue **â€œPhase 4Â·Â½ Complete â†’ start Phase 5 FEâ€** created

---

## 5 â€” Phase 5 Â· **Feature Engineering**<a name="5-phase-5--feature-engineering"></a>

> All â€œcolumn-craftingâ€ lives in **[`feature_engineering.py`](src/Feature%20Engineering/feature_engineering.py)**.  
> The `FeatureEngineer` class is a **buffet**: every classic transform is baked-in but
> disabled by defaultâ€”switch items on via kwargs or a small JSON/YAML config.

---

### 5Â·A Menu of Built-in Options<a name="5-phase-5--feature-engineering"></a>

| Category                  | Turn on with â‡¢                                                                | Notes                           |
| ------------------------- | ----------------------------------------------------------------------------- | ------------------------------- | ------------------------------- | ------ | ------ | -------- | ------ | ------------------------- |
| Numeric scalers           | `numeric_scaler="standard                                                     | minmax                          | robust                          | maxabs | normal | quantile | none"` |                           |
| Power / log               | `numeric_power="yeo                                                           | boxcox                          | quantile"`Â·`log_cols=["price"]` |        |
| Binning                   | `quantile_bins={"age":4}` or `binning={"age":{"bins":5,"strategy":"kmeans"}}` |                                 |
| Polynomial & interactions | `polynomial_degree=2` Â· `interactions=True`                                   |                                 |
| Rare grouping             | `rare_threshold=0.01 # 1 %`                                                   | merges into `__rare__`          |
| Cat encoders              | `cat_encoder="onehot                                                          | ordinal                         | target                          | woe    | hash   | freq     | none"` | Target/WOE need `target=` |
| Text vecs                 | `text_vectorizer="tfidf                                                       | count                           | hashing"`Â·`text_cols=[â€¦]`       |        |
| Datetime expand           | `datetime_cols=[â€¦]`                                                           | Y/M/D/DOW/HR                    |
| Cyclical sinâ€“cos          | `cyclical_cols={"month":12,"dow":7}`                                          |                                 |
| Date deltas               | `date_delta_cols={"signup":"today"}`                                          | days-since                      |
| Aggregations              | `aggregations={"cust_id":["amt_mean","amt_sum"]}`                             | group-by roll-ups               |
| SMOTE                     | `sampler="smote"`                                                             | oversample during **fit**       |
| Custom plug-ins           | `custom_steps=[my_func]`                                                      | any `pd.DataFrameâ†’pd.DataFrame` |

---

### 5Â·B Quick Recipes

**Minimal**

```python
fe = FeatureEngineer(target="is_fraud").fit(df)
X  = fe.transform(df)
fe.save()  # âœ models/preprocessor.joblib
```

**Heavy stack**

```python
fe = FeatureEngineer(
        target="is_churn",
        numeric_scaler="robust",
        numeric_power="yeo",
        log_cols=["revenue"],
        quantile_bins={"age":4},
        cat_encoder="hash",
        rare_threshold=10,
        text_vectorizer="tfidf",
        text_cols=["review"],
        datetime_cols=["last_login"],
        cyclical_cols={"hour":24},
        polynomial_degree=2,
        sampler="smote"
     ).fit(df, df.is_churn)
X = fe.transform(df); fe.save()
```

**CLI**

```bash
python -m Feature_Engineering.feature_engineering \
       --data data/processed/scaled.parquet \
       --target is_churn \
       --numeric_scaler robust \
       --log_cols revenue
```

---

### 5Â·C Artefacts

| File                                | Role                                       |
| ----------------------------------- | ------------------------------------------ |
| `models/preprocessor.joblib`        | Frozen transform pipeline (+SMOTE if used) |
| `models/preprocessor_manifest.json` | SHA-256 + config snapshot                  |
| `reports/feature_shape.txt`         | Dense/-sparse shape & nnz %                |

---

### 5Â·D Exit Checklist

- [ ] Pipeline fitted on **train + val** only (no test leakage)
- [ ] `preprocessor.joblib` tracked in DVC / registry
- [ ] Shape & sparsity logged
- [ ] No silent drops of cat/text columns
- [ ] Custom plug-in tests pass

---

### 5Â·E Custom Feature-Engineering Plug-ins<a name="5e-custom--advanced-plug-ins"></a>

Not every transform you need will fit the built-ins.
`FeatureEngineer` therefore accepts a list of **arbitrary callables**:

```python
custom_steps = [my_func1, my_func2, â€¦]   # each:  pd.DataFrame â†’ pd.DataFrame
```

They run **after** the standard ColumnTransformer, so they can read/write any
columns already produced by scaling, encoders, text vectors, etc.

#### Example â€“ domain ratios & log-tenure

```python
import numpy as np, pandas as pd
from Feature_Engineering.feature_engineering import FeatureEngineer

def add_ratios(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    df["spend_per_visit"] = df["total_spend"] / (df["num_visits"].clip(lower=1))
    df["log_tenure"]      = np.log1p(df["tenure_days"])
    return df

fe = FeatureEngineer(
        target="is_churn",
        numeric_scaler="standard",
        custom_steps=[add_ratios]
     ).fit(train_df, train_df.is_churn)

X_train = fe.transform(train_df)
fe.save()   # new columns now frozen into pre-processor
```

_Guidelines_

- Return **all original columns + new ones** (donâ€™t drop unless intentional).
- Keep it **pure**: no I/O, no global stateâ€”makes the pipeline portable.
- If you need parameters, wrap them in a closure or `functools.partial`.
- Add unit-tests in `tests/test_custom_steps.py` so the Phase-5 exit checklist
  can verify they still work after refactors.

> Once your custom step is serialised inside `preprocessor.joblib`, every model
> in Phase 6 will use it automaticallyâ€”no extra code paths to maintain.

## ğŸ†• Phase 5Â·Â½ â€” **Baseline Benchmarking & & Pre-Processor Freeze** <a name="5.5-phase-baseline-freeze"></a>

> _Glue_ between **Feature Engineering** and **Model Design**.  
> Freezes deterministic splits, prevents leakage, and sets a â€œbeat-thatâ€ baseline.

| Sub-step                          | Goal                                                        | Artefact(s)                                                    |
| --------------------------------- | ----------------------------------------------------------- | -------------------------------------------------------------- |
| **5Â·0 Train / Val / Test Split**  | Comparable, leak-free folds                                 | `data/splits/{train,val,test}.parquet`â€‚+â€‚`split_manifest.json` |
| **5Â·1 Stratification / Grouping** | Preserve class proportions or entity boundaries             | implemented inside **`split_and_baseline.py`**                 |
| **5Â·2 Baseline Model(s)**         | Majority-class, mean regressor, or random ranker            | `reports/baseline/baseline_metrics.json`                       |
| **5Â·3 Sanity Checks**             | Duplicate-row catch, leakage sniff, feature-drift check     | pipeline aborts on failure                                     |
| **5Â·4 Data-Pipeline Freeze**      | Persist the _fitted_ pre-processor used to build the splits | `models/preprocessor.joblib`â€‚+â€‚`preprocessor_manifest.json`    |

#### ğŸ“œ Code location

`src/Data Cleaning/split_and_baseline.py` â€“ single class **`SplitAndBaseline`**
(`fit â†’ split â†’ baseline â†’ checks â†’ freeze`).

```bash
# run end-to-end
python -m Data_Cleaning.split_and_baseline \
       --target is_churn \
       --stratify \
       --seed 42

```

```mermaid
flowchart TD
    A[0 Â· LOAD<br>processed.parquet] --> B[1 Â· STRAT / SPLIT]
    B --> C[2 Â· BASELINE<br>majority / mean]
    C --> D[3 Â· SANITY CHECKS]
    D --> E[4 Â· FREEZE PREPROCESSOR<br>+ SHA manifest]
```

The script:

1. Loads **`data/processed/scaled.parquet`**
2. Creates deterministic splits (stratified if flagged)
3. Computes & stores baseline metrics
4. Runs fast-fail leakage / duplication checks
5. Saves a SHA-stamped `preprocessor.joblib` + manifest

> **Exit criterion:** anyone can clone the repo, run `make baseline`,
> and reproduce the metrics within **Â± 0.01**.
> If the script fails, fix the issues before proceeding to Phase 6.

---
